{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4083802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f630d_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >A</th>\n",
       "      <th class=\"col_heading level0 col1\" >B</th>\n",
       "      <th class=\"col_heading level0 col2\" >C</th>\n",
       "      <th class=\"col_heading level0 col3\" >D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f630d_row0_col0\" class=\"data row0 col0\" >1.764100</td>\n",
       "      <td id=\"T_f630d_row0_col1\" class=\"data row0 col1\" >0.877900</td>\n",
       "      <td id=\"T_f630d_row0_col2\" class=\"data row0 col2\" >0.837900</td>\n",
       "      <td id=\"T_f630d_row0_col3\" class=\"data row0 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f630d_row1_col0\" class=\"data row1 col0\" >0.400200</td>\n",
       "      <td id=\"T_f630d_row1_col1\" class=\"data row1 col1\" >-0.854900</td>\n",
       "      <td id=\"T_f630d_row1_col2\" class=\"data row1 col2\" >0.096100</td>\n",
       "      <td id=\"T_f630d_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f630d_row2_col0\" class=\"data row2 col0\" >0.978700</td>\n",
       "      <td id=\"T_f630d_row2_col1\" class=\"data row2 col1\" >-1.554500</td>\n",
       "      <td id=\"T_f630d_row2_col2\" class=\"data row2 col2\" >0.976500</td>\n",
       "      <td id=\"T_f630d_row2_col3\" class=\"data row2 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f630d_row3_col0\" class=\"data row3 col0\" >2.240900</td>\n",
       "      <td id=\"T_f630d_row3_col1\" class=\"data row3 col1\" >-1.425500</td>\n",
       "      <td id=\"T_f630d_row3_col2\" class=\"data row3 col2\" >0.468700</td>\n",
       "      <td id=\"T_f630d_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f630d_row4_col0\" class=\"data row4 col0\" >1.867600</td>\n",
       "      <td id=\"T_f630d_row4_col1\" class=\"data row4 col1\" >-0.803900</td>\n",
       "      <td id=\"T_f630d_row4_col2\" class=\"data row4 col2\" >0.976800</td>\n",
       "      <td id=\"T_f630d_row4_col3\" class=\"data row4 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f630d_row5_col0\" class=\"data row5 col0\" >-0.977300</td>\n",
       "      <td id=\"T_f630d_row5_col1\" class=\"data row5 col1\" >-1.937900</td>\n",
       "      <td id=\"T_f630d_row5_col2\" class=\"data row5 col2\" >0.604800</td>\n",
       "      <td id=\"T_f630d_row5_col3\" class=\"data row5 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f630d_row6_col0\" class=\"data row6 col0\" >0.950100</td>\n",
       "      <td id=\"T_f630d_row6_col1\" class=\"data row6 col1\" >-0.901100</td>\n",
       "      <td id=\"T_f630d_row6_col2\" class=\"data row6 col2\" >0.739300</td>\n",
       "      <td id=\"T_f630d_row6_col3\" class=\"data row6 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f630d_row7_col0\" class=\"data row7 col0\" >-0.151400</td>\n",
       "      <td id=\"T_f630d_row7_col1\" class=\"data row7 col1\" >-0.889400</td>\n",
       "      <td id=\"T_f630d_row7_col2\" class=\"data row7 col2\" >0.039200</td>\n",
       "      <td id=\"T_f630d_row7_col3\" class=\"data row7 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f630d_row8_col0\" class=\"data row8 col0\" >-0.103200</td>\n",
       "      <td id=\"T_f630d_row8_col1\" class=\"data row8 col1\" >-1.763800</td>\n",
       "      <td id=\"T_f630d_row8_col2\" class=\"data row8 col2\" >0.282800</td>\n",
       "      <td id=\"T_f630d_row8_col3\" class=\"data row8 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f630d_row9_col0\" class=\"data row9 col0\" >0.410600</td>\n",
       "      <td id=\"T_f630d_row9_col1\" class=\"data row9 col1\" >-1.862000</td>\n",
       "      <td id=\"T_f630d_row9_col2\" class=\"data row9 col2\" >0.120200</td>\n",
       "      <td id=\"T_f630d_row9_col3\" class=\"data row9 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f630d_row10_col0\" class=\"data row10 col0\" >0.144000</td>\n",
       "      <td id=\"T_f630d_row10_col1\" class=\"data row10 col1\" >-1.621000</td>\n",
       "      <td id=\"T_f630d_row10_col2\" class=\"data row10 col2\" >0.296100</td>\n",
       "      <td id=\"T_f630d_row10_col3\" class=\"data row10 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f630d_row11_col0\" class=\"data row11 col0\" >1.454300</td>\n",
       "      <td id=\"T_f630d_row11_col1\" class=\"data row11 col1\" >-1.547900</td>\n",
       "      <td id=\"T_f630d_row11_col2\" class=\"data row11 col2\" >0.118700</td>\n",
       "      <td id=\"T_f630d_row11_col3\" class=\"data row11 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f630d_row12_col0\" class=\"data row12 col0\" >0.761000</td>\n",
       "      <td id=\"T_f630d_row12_col1\" class=\"data row12 col1\" >-1.155600</td>\n",
       "      <td id=\"T_f630d_row12_col2\" class=\"data row12 col2\" >0.318000</td>\n",
       "      <td id=\"T_f630d_row12_col3\" class=\"data row12 col3\" >2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f630d_row13_col0\" class=\"data row13 col0\" >0.121700</td>\n",
       "      <td id=\"T_f630d_row13_col1\" class=\"data row13 col1\" >-1.422700</td>\n",
       "      <td id=\"T_f630d_row13_col2\" class=\"data row13 col2\" >0.414300</td>\n",
       "      <td id=\"T_f630d_row13_col3\" class=\"data row13 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f630d_row14_col0\" class=\"data row14 col0\" >0.443900</td>\n",
       "      <td id=\"T_f630d_row14_col1\" class=\"data row14 col1\" >2.454500</td>\n",
       "      <td id=\"T_f630d_row14_col2\" class=\"data row14 col2\" >0.064100</td>\n",
       "      <td id=\"T_f630d_row14_col3\" class=\"data row14 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f630d_row15_col0\" class=\"data row15 col0\" >0.333700</td>\n",
       "      <td id=\"T_f630d_row15_col1\" class=\"data row15 col1\" >-1.892400</td>\n",
       "      <td id=\"T_f630d_row15_col2\" class=\"data row15 col2\" >0.692500</td>\n",
       "      <td id=\"T_f630d_row15_col3\" class=\"data row15 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f630d_row16_col0\" class=\"data row16 col0\" >1.494100</td>\n",
       "      <td id=\"T_f630d_row16_col1\" class=\"data row16 col1\" >-1.765700</td>\n",
       "      <td id=\"T_f630d_row16_col2\" class=\"data row16 col2\" >0.566600</td>\n",
       "      <td id=\"T_f630d_row16_col3\" class=\"data row16 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f630d_row17_col0\" class=\"data row17 col0\" >-0.205200</td>\n",
       "      <td id=\"T_f630d_row17_col1\" class=\"data row17 col1\" >-1.824100</td>\n",
       "      <td id=\"T_f630d_row17_col2\" class=\"data row17 col2\" >0.265400</td>\n",
       "      <td id=\"T_f630d_row17_col3\" class=\"data row17 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f630d_row18_col0\" class=\"data row18 col0\" >0.313100</td>\n",
       "      <td id=\"T_f630d_row18_col1\" class=\"data row18 col1\" >-0.941300</td>\n",
       "      <td id=\"T_f630d_row18_col2\" class=\"data row18 col2\" >0.523200</td>\n",
       "      <td id=\"T_f630d_row18_col3\" class=\"data row18 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f630d_row19_col0\" class=\"data row19 col0\" >-0.854100</td>\n",
       "      <td id=\"T_f630d_row19_col1\" class=\"data row19 col1\" >-1.707900</td>\n",
       "      <td id=\"T_f630d_row19_col2\" class=\"data row19 col2\" >0.093900</td>\n",
       "      <td id=\"T_f630d_row19_col3\" class=\"data row19 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_f630d_row20_col0\" class=\"data row20 col0\" >-2.553000</td>\n",
       "      <td id=\"T_f630d_row20_col1\" class=\"data row20 col1\" >-1.372100</td>\n",
       "      <td id=\"T_f630d_row20_col2\" class=\"data row20 col2\" >0.575900</td>\n",
       "      <td id=\"T_f630d_row20_col3\" class=\"data row20 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_f630d_row21_col0\" class=\"data row21 col0\" >0.653600</td>\n",
       "      <td id=\"T_f630d_row21_col1\" class=\"data row21 col1\" >-1.719700</td>\n",
       "      <td id=\"T_f630d_row21_col2\" class=\"data row21 col2\" >0.929300</td>\n",
       "      <td id=\"T_f630d_row21_col3\" class=\"data row21 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_f630d_row22_col0\" class=\"data row22 col0\" >0.864400</td>\n",
       "      <td id=\"T_f630d_row22_col1\" class=\"data row22 col1\" >-1.826900</td>\n",
       "      <td id=\"T_f630d_row22_col2\" class=\"data row22 col2\" >0.318600</td>\n",
       "      <td id=\"T_f630d_row22_col3\" class=\"data row22 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_f630d_row23_col0\" class=\"data row23 col0\" >-0.742200</td>\n",
       "      <td id=\"T_f630d_row23_col1\" class=\"data row23 col1\" >-1.883000</td>\n",
       "      <td id=\"T_f630d_row23_col2\" class=\"data row23 col2\" >0.667400</td>\n",
       "      <td id=\"T_f630d_row23_col3\" class=\"data row23 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_f630d_row24_col0\" class=\"data row24 col0\" >2.269800</td>\n",
       "      <td id=\"T_f630d_row24_col1\" class=\"data row24 col1\" >-0.931900</td>\n",
       "      <td id=\"T_f630d_row24_col2\" class=\"data row24 col2\" >0.131800</td>\n",
       "      <td id=\"T_f630d_row24_col3\" class=\"data row24 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_f630d_row25_col0\" class=\"data row25 col0\" >-1.454400</td>\n",
       "      <td id=\"T_f630d_row25_col1\" class=\"data row25 col1\" >-1.851300</td>\n",
       "      <td id=\"T_f630d_row25_col2\" class=\"data row25 col2\" >0.716300</td>\n",
       "      <td id=\"T_f630d_row25_col3\" class=\"data row25 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_f630d_row26_col0\" class=\"data row26 col0\" >0.045800</td>\n",
       "      <td id=\"T_f630d_row26_col1\" class=\"data row26 col1\" >-1.781100</td>\n",
       "      <td id=\"T_f630d_row26_col2\" class=\"data row26 col2\" >0.289400</td>\n",
       "      <td id=\"T_f630d_row26_col3\" class=\"data row26 col3\" >2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_f630d_row27_col0\" class=\"data row27 col0\" >-0.187200</td>\n",
       "      <td id=\"T_f630d_row27_col1\" class=\"data row27 col1\" >-1.540000</td>\n",
       "      <td id=\"T_f630d_row27_col2\" class=\"data row27 col2\" >0.183200</td>\n",
       "      <td id=\"T_f630d_row27_col3\" class=\"data row27 col3\" >3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_f630d_row28_col0\" class=\"data row28 col0\" >1.532800</td>\n",
       "      <td id=\"T_f630d_row28_col1\" class=\"data row28 col1\" >-0.279700</td>\n",
       "      <td id=\"T_f630d_row28_col2\" class=\"data row28 col2\" >0.586500</td>\n",
       "      <td id=\"T_f630d_row28_col3\" class=\"data row28 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f630d_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_f630d_row29_col0\" class=\"data row29 col0\" >1.469400</td>\n",
       "      <td id=\"T_f630d_row29_col1\" class=\"data row29 col1\" >-1.897900</td>\n",
       "      <td id=\"T_f630d_row29_col2\" class=\"data row29 col2\" >0.020100</td>\n",
       "      <td id=\"T_f630d_row29_col3\" class=\"data row29 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fcead824910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import wuml\n",
    "\n",
    "data = wuml.wData(xpath='../../data/shap_regress_example_mix_distributions.csv', batch_size=20, \n",
    "\t\t\t\t\tlabel_type='continuous', label_column_name='label', first_row_is_label=True)\n",
    "data.df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "862ff7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_501b1_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >A</th>\n",
       "      <th class=\"col_heading level0 col1\" >B</th>\n",
       "      <th class=\"col_heading level0 col2\" >C</th>\n",
       "      <th class=\"col_heading level0 col3\" >D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_501b1_row0_col0\" class=\"data row0 col0\" >0.846862</td>\n",
       "      <td id=\"T_501b1_row0_col1\" class=\"data row0 col1\" >0.949368</td>\n",
       "      <td id=\"T_501b1_row0_col2\" class=\"data row0 col2\" >0.881000</td>\n",
       "      <td id=\"T_501b1_row0_col3\" class=\"data row0 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_501b1_row1_col0\" class=\"data row1 col0\" >0.473788</td>\n",
       "      <td id=\"T_501b1_row1_col1\" class=\"data row1 col1\" >0.754489</td>\n",
       "      <td id=\"T_501b1_row1_col2\" class=\"data row1 col2\" >0.154882</td>\n",
       "      <td id=\"T_501b1_row1_col3\" class=\"data row1 col3\" >0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_501b1_row2_col0\" class=\"data row2 col0\" >0.653447</td>\n",
       "      <td id=\"T_501b1_row2_col1\" class=\"data row2 col1\" >0.435621</td>\n",
       "      <td id=\"T_501b1_row2_col2\" class=\"data row2 col2\" >0.952684</td>\n",
       "      <td id=\"T_501b1_row2_col3\" class=\"data row2 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_501b1_row3_col0\" class=\"data row3 col0\" >0.924730</td>\n",
       "      <td id=\"T_501b1_row3_col1\" class=\"data row3 col1\" >0.504119</td>\n",
       "      <td id=\"T_501b1_row3_col2\" class=\"data row3 col2\" >0.559689</td>\n",
       "      <td id=\"T_501b1_row3_col3\" class=\"data row3 col3\" >0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_501b1_row4_col0\" class=\"data row4 col0\" >0.866509</td>\n",
       "      <td id=\"T_501b1_row4_col1\" class=\"data row4 col1\" >0.770932</td>\n",
       "      <td id=\"T_501b1_row4_col2\" class=\"data row4 col2\" >0.952820</td>\n",
       "      <td id=\"T_501b1_row4_col3\" class=\"data row4 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_501b1_row5_col0\" class=\"data row5 col0\" >0.128584</td>\n",
       "      <td id=\"T_501b1_row5_col1\" class=\"data row5 col1\" >0.238832</td>\n",
       "      <td id=\"T_501b1_row5_col2\" class=\"data row5 col2\" >0.689800</td>\n",
       "      <td id=\"T_501b1_row5_col3\" class=\"data row5 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_501b1_row6_col0\" class=\"data row6 col0\" >0.645100</td>\n",
       "      <td id=\"T_501b1_row6_col1\" class=\"data row6 col1\" >0.738690</td>\n",
       "      <td id=\"T_501b1_row6_col2\" class=\"data row6 col2\" >0.815942</td>\n",
       "      <td id=\"T_501b1_row6_col3\" class=\"data row6 col3\" >0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_501b1_row7_col0\" class=\"data row7 col0\" >0.304919</td>\n",
       "      <td id=\"T_501b1_row7_col1\" class=\"data row7 col1\" >0.742772</td>\n",
       "      <td id=\"T_501b1_row7_col2\" class=\"data row7 col2\" >0.092154</td>\n",
       "      <td id=\"T_501b1_row7_col3\" class=\"data row7 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_501b1_row8_col0\" class=\"data row8 col0\" >0.318543</td>\n",
       "      <td id=\"T_501b1_row8_col1\" class=\"data row8 col1\" >0.324535</td>\n",
       "      <td id=\"T_501b1_row8_col2\" class=\"data row8 col2\" >0.381273</td>\n",
       "      <td id=\"T_501b1_row8_col3\" class=\"data row8 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_501b1_row9_col0\" class=\"data row9 col0\" >0.477124</td>\n",
       "      <td id=\"T_501b1_row9_col1\" class=\"data row9 col1\" >0.275015</td>\n",
       "      <td id=\"T_501b1_row9_col2\" class=\"data row9 col2\" >0.184256</td>\n",
       "      <td id=\"T_501b1_row9_col3\" class=\"data row9 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_501b1_row10_col0\" class=\"data row10 col0\" >0.392546</td>\n",
       "      <td id=\"T_501b1_row10_col1\" class=\"data row10 col1\" >0.399961</td>\n",
       "      <td id=\"T_501b1_row10_col2\" class=\"data row10 col2\" >0.396213</td>\n",
       "      <td id=\"T_501b1_row10_col3\" class=\"data row10 col3\" >0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_501b1_row11_col0\" class=\"data row11 col0\" >0.779488</td>\n",
       "      <td id=\"T_501b1_row11_col1\" class=\"data row11 col1\" >0.439158</td>\n",
       "      <td id=\"T_501b1_row11_col2\" class=\"data row11 col2\" >0.182403</td>\n",
       "      <td id=\"T_501b1_row11_col3\" class=\"data row11 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_501b1_row12_col0\" class=\"data row12 col0\" >0.588130</td>\n",
       "      <td id=\"T_501b1_row12_col1\" class=\"data row12 col1\" >0.636504</td>\n",
       "      <td id=\"T_501b1_row12_col2\" class=\"data row12 col2\" >0.420120</td>\n",
       "      <td id=\"T_501b1_row12_col3\" class=\"data row12 col3\" >0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_501b1_row13_col0\" class=\"data row13 col0\" >0.385636</td>\n",
       "      <td id=\"T_501b1_row13_col1\" class=\"data row13 col1\" >0.505583</td>\n",
       "      <td id=\"T_501b1_row13_col2\" class=\"data row13 col2\" >0.512922</td>\n",
       "      <td id=\"T_501b1_row13_col3\" class=\"data row13 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_501b1_row14_col0\" class=\"data row14 col0\" >0.487806</td>\n",
       "      <td id=\"T_501b1_row14_col1\" class=\"data row14 col1\" >0.983269</td>\n",
       "      <td id=\"T_501b1_row14_col2\" class=\"data row14 col2\" >0.118055</td>\n",
       "      <td id=\"T_501b1_row14_col3\" class=\"data row14 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_501b1_row15_col0\" class=\"data row15 col0\" >0.452486</td>\n",
       "      <td id=\"T_501b1_row15_col1\" class=\"data row15 col1\" >0.260273</td>\n",
       "      <td id=\"T_501b1_row15_col2\" class=\"data row15 col2\" >0.776329</td>\n",
       "      <td id=\"T_501b1_row15_col3\" class=\"data row15 col3\" >0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_501b1_row16_col0\" class=\"data row16 col0\" >0.788836</td>\n",
       "      <td id=\"T_501b1_row16_col1\" class=\"data row16 col1\" >0.323554</td>\n",
       "      <td id=\"T_501b1_row16_col2\" class=\"data row16 col2\" >0.650872</td>\n",
       "      <td id=\"T_501b1_row16_col3\" class=\"data row16 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_501b1_row17_col0\" class=\"data row17 col0\" >0.290086</td>\n",
       "      <td id=\"T_501b1_row17_col1\" class=\"data row17 col1\" >0.293811</td>\n",
       "      <td id=\"T_501b1_row17_col2\" class=\"data row17 col2\" >0.361324</td>\n",
       "      <td id=\"T_501b1_row17_col3\" class=\"data row17 col3\" >0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_501b1_row18_col0\" class=\"data row18 col0\" >0.445905</td>\n",
       "      <td id=\"T_501b1_row18_col1\" class=\"data row18 col1\" >0.724242</td>\n",
       "      <td id=\"T_501b1_row18_col2\" class=\"data row18 col2\" >0.608615</td>\n",
       "      <td id=\"T_501b1_row18_col3\" class=\"data row18 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_501b1_row19_col0\" class=\"data row19 col0\" >0.147752</td>\n",
       "      <td id=\"T_501b1_row19_col1\" class=\"data row19 col1\" >0.353726</td>\n",
       "      <td id=\"T_501b1_row19_col2\" class=\"data row19 col2\" >0.152255</td>\n",
       "      <td id=\"T_501b1_row19_col3\" class=\"data row19 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_501b1_row20_col0\" class=\"data row20 col0\" >0.019402</td>\n",
       "      <td id=\"T_501b1_row20_col1\" class=\"data row20 col1\" >0.531787</td>\n",
       "      <td id=\"T_501b1_row20_col2\" class=\"data row20 col2\" >0.660254</td>\n",
       "      <td id=\"T_501b1_row20_col3\" class=\"data row20 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_501b1_row21_col0\" class=\"data row21 col0\" >0.554629</td>\n",
       "      <td id=\"T_501b1_row21_col1\" class=\"data row21 col1\" >0.347518</td>\n",
       "      <td id=\"T_501b1_row21_col2\" class=\"data row21 col2\" >0.929853</td>\n",
       "      <td id=\"T_501b1_row21_col3\" class=\"data row21 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_501b1_row22_col0\" class=\"data row22 col0\" >0.619644</td>\n",
       "      <td id=\"T_501b1_row22_col1\" class=\"data row22 col1\" >0.292408</td>\n",
       "      <td id=\"T_501b1_row22_col2\" class=\"data row22 col2\" >0.420761</td>\n",
       "      <td id=\"T_501b1_row22_col3\" class=\"data row22 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_501b1_row23_col0\" class=\"data row23 col0\" >0.167262</td>\n",
       "      <td id=\"T_501b1_row23_col1\" class=\"data row23 col1\" >0.264798</td>\n",
       "      <td id=\"T_501b1_row23_col2\" class=\"data row23 col2\" >0.752787</td>\n",
       "      <td id=\"T_501b1_row23_col3\" class=\"data row23 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_501b1_row24_col0\" class=\"data row24 col0\" >0.928401</td>\n",
       "      <td id=\"T_501b1_row24_col1\" class=\"data row24 col1\" >0.727679</td>\n",
       "      <td id=\"T_501b1_row24_col2\" class=\"data row24 col2\" >0.198653</td>\n",
       "      <td id=\"T_501b1_row24_col3\" class=\"data row24 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_501b1_row25_col0\" class=\"data row25 col0\" >0.074158</td>\n",
       "      <td id=\"T_501b1_row25_col1\" class=\"data row25 col1\" >0.280277</td>\n",
       "      <td id=\"T_501b1_row25_col2\" class=\"data row25 col2\" >0.797227</td>\n",
       "      <td id=\"T_501b1_row25_col3\" class=\"data row25 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_501b1_row26_col0\" class=\"data row26 col0\" >0.362423</td>\n",
       "      <td id=\"T_501b1_row26_col1\" class=\"data row26 col1\" >0.315629</td>\n",
       "      <td id=\"T_501b1_row26_col2\" class=\"data row26 col2\" >0.388723</td>\n",
       "      <td id=\"T_501b1_row26_col3\" class=\"data row26 col3\" >0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_501b1_row27_col0\" class=\"data row27 col0\" >0.295003</td>\n",
       "      <td id=\"T_501b1_row27_col1\" class=\"data row27 col1\" >0.443390</td>\n",
       "      <td id=\"T_501b1_row27_col2\" class=\"data row27 col2\" >0.262668</td>\n",
       "      <td id=\"T_501b1_row27_col3\" class=\"data row27 col3\" >0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_501b1_row28_col0\" class=\"data row28 col0\" >0.797734</td>\n",
       "      <td id=\"T_501b1_row28_col1\" class=\"data row28 col1\" >0.884421</td>\n",
       "      <td id=\"T_501b1_row28_col2\" class=\"data row28 col2\" >0.671041</td>\n",
       "      <td id=\"T_501b1_row28_col3\" class=\"data row28 col3\" >0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_501b1_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_501b1_row29_col0\" class=\"data row29 col0\" >0.783058</td>\n",
       "      <td id=\"T_501b1_row29_col1\" class=\"data row29 col1\" >0.257640</td>\n",
       "      <td id=\"T_501b1_row29_col2\" class=\"data row29 col2\" >0.074426</td>\n",
       "      <td id=\"T_501b1_row29_col3\" class=\"data row29 col3\" >0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fcf94181790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Udata = wuml.use_cdf_to_map_data_between_0_and_1(data, output_type_name='wData')\n",
    "Udata.df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430879df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Info:\n",
      "\tLearning rate: 0.001\n",
      "\tMax number of epochs: 1000\n",
      "\tCost Function: mse\n",
      "\tTrain Loop Callback: None\n",
      "\tCuda Available: True\n",
      "\tNetwork Structure\n",
      "\t\tLinear(in_features=4, out_features=100, bias=True) , relu\n",
      "\t\tLinear(in_features=100, out_features=100, bias=True) , relu\n",
      "\t\tLinear(in_features=100, out_features=1, bias=True) , none\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 139, Avg Loss: 47.2592, Learning Rate: 0.00100000\tepoch: 2, Avg Loss: 1224.1724, Learning Rate: 0.00100000\tepoch: 3, Avg Loss: 1258.7435, Learning Rate: 0.00100000\tepoch: 4, Avg Loss: 1261.2665, Learning Rate: 0.00100000\tepoch: 5, Avg Loss: 1257.8422, Learning Rate: 0.00100000\tepoch: 6, Avg Loss: 1168.7086, Learning Rate: 0.00100000\tepoch: 7, Avg Loss: 1226.0413, Learning Rate: 0.00100000\tepoch: 8, Avg Loss: 1153.0009, Learning Rate: 0.00100000\tepoch: 9, Avg Loss: 1142.4724, Learning Rate: 0.00100000\tepoch: 10, Avg Loss: 1181.5177, Learning Rate: 0.00100000\tepoch: 11, Avg Loss: 1146.4163, Learning Rate: 0.00100000\tepoch: 12, Avg Loss: 1157.9622, Learning Rate: 0.00100000\tepoch: 13, Avg Loss: 1093.0511, Learning Rate: 0.00100000\tepoch: 14, Avg Loss: 1081.5175, Learning Rate: 0.00100000\tepoch: 15, Avg Loss: 1114.7003, Learning Rate: 0.00100000\tepoch: 16, Avg Loss: 1039.4012, Learning Rate: 0.00100000\tepoch: 17, Avg Loss: 1098.4674, Learning Rate: 0.00100000\tepoch: 18, Avg Loss: 1038.8366, Learning Rate: 0.00100000\tepoch: 19, Avg Loss: 1007.5022, Learning Rate: 0.00100000\tepoch: 20, Avg Loss: 1125.4674, Learning Rate: 0.00100000\tepoch: 21, Avg Loss: 1036.6541, Learning Rate: 0.00100000\tepoch: 22, Avg Loss: 1030.6425, Learning Rate: 0.00100000\tepoch: 23, Avg Loss: 987.1973, Learning Rate: 0.00100000\tepoch: 24, Avg Loss: 1029.5835, Learning Rate: 0.00100000\tepoch: 25, Avg Loss: 956.8388, Learning Rate: 0.00100000\tepoch: 26, Avg Loss: 972.5630, Learning Rate: 0.00100000\tepoch: 27, Avg Loss: 929.4921, Learning Rate: 0.00100000\tepoch: 28, Avg Loss: 838.5008, Learning Rate: 0.00100000\tepoch: 29, Avg Loss: 875.6516, Learning Rate: 0.00100000\tepoch: 30, Avg Loss: 864.5623, Learning Rate: 0.00100000\tepoch: 31, Avg Loss: 791.9199, Learning Rate: 0.00100000\tepoch: 32, Avg Loss: 784.6650, Learning Rate: 0.00100000\tepoch: 33, Avg Loss: 769.5458, Learning Rate: 0.00100000\tepoch: 34, Avg Loss: 749.3619, Learning Rate: 0.00100000\tepoch: 35, Avg Loss: 648.2719, Learning Rate: 0.00100000\tepoch: 36, Avg Loss: 620.7999, Learning Rate: 0.00100000\tepoch: 37, Avg Loss: 595.3542, Learning Rate: 0.00100000\tepoch: 38, Avg Loss: 558.7561, Learning Rate: 0.00100000\tepoch: 39, Avg Loss: 531.1571, Learning Rate: 0.00100000\tepoch: 40, Avg Loss: 514.1749, Learning Rate: 0.00100000\tepoch: 41, Avg Loss: 466.3440, Learning Rate: 0.00100000\tepoch: 42, Avg Loss: 465.6125, Learning Rate: 0.00100000\tepoch: 43, Avg Loss: 450.2000, Learning Rate: 0.00100000\tepoch: 44, Avg Loss: 403.7370, Learning Rate: 0.00100000\tepoch: 45, Avg Loss: 348.3793, Learning Rate: 0.00100000\tepoch: 46, Avg Loss: 341.8099, Learning Rate: 0.00100000\tepoch: 47, Avg Loss: 349.0983, Learning Rate: 0.00100000\tepoch: 48, Avg Loss: 325.4159, Learning Rate: 0.00100000\tepoch: 49, Avg Loss: 315.1752, Learning Rate: 0.00100000\tepoch: 50, Avg Loss: 275.5075, Learning Rate: 0.00100000\tepoch: 51, Avg Loss: 313.7061, Learning Rate: 0.00100000\tepoch: 52, Avg Loss: 283.3686, Learning Rate: 0.00100000\tepoch: 53, Avg Loss: 251.0998, Learning Rate: 0.00100000\tepoch: 54, Avg Loss: 293.4619, Learning Rate: 0.00100000\tepoch: 55, Avg Loss: 276.0041, Learning Rate: 0.00100000\tepoch: 56, Avg Loss: 275.0419, Learning Rate: 0.00100000\tepoch: 57, Avg Loss: 267.4815, Learning Rate: 0.00100000\tepoch: 58, Avg Loss: 278.1436, Learning Rate: 0.00100000\tepoch: 59, Avg Loss: 242.9058, Learning Rate: 0.00100000\tepoch: 60, Avg Loss: 271.2486, Learning Rate: 0.00100000\tepoch: 61, Avg Loss: 237.3980, Learning Rate: 0.00100000\tepoch: 62, Avg Loss: 261.0423, Learning Rate: 0.00100000\tepoch: 63, Avg Loss: 289.4807, Learning Rate: 0.00100000\tepoch: 64, Avg Loss: 269.9943, Learning Rate: 0.00100000\tepoch: 65, Avg Loss: 252.2146, Learning Rate: 0.00100000\tepoch: 66, Avg Loss: 227.0804, Learning Rate: 0.00100000\tepoch: 67, Avg Loss: 237.1335, Learning Rate: 0.00100000\tepoch: 68, Avg Loss: 223.7604, Learning Rate: 0.00100000\tepoch: 69, Avg Loss: 239.3724, Learning Rate: 0.00100000\tepoch: 70, Avg Loss: 219.5955, Learning Rate: 0.00100000\tepoch: 71, Avg Loss: 223.4077, Learning Rate: 0.00100000\tepoch: 72, Avg Loss: 231.4482, Learning Rate: 0.00100000\tepoch: 73, Avg Loss: 229.6518, Learning Rate: 0.00100000\tepoch: 74, Avg Loss: 224.7381, Learning Rate: 0.00100000\tepoch: 75, Avg Loss: 244.1399, Learning Rate: 0.00100000\tepoch: 76, Avg Loss: 234.9245, Learning Rate: 0.00100000\tepoch: 77, Avg Loss: 201.7740, Learning Rate: 0.00100000\tepoch: 78, Avg Loss: 209.7256, Learning Rate: 0.00100000\tepoch: 79, Avg Loss: 208.0182, Learning Rate: 0.00100000\tepoch: 80, Avg Loss: 195.8503, Learning Rate: 0.00100000\tepoch: 81, Avg Loss: 213.8564, Learning Rate: 0.00100000\tepoch: 82, Avg Loss: 222.6380, Learning Rate: 0.00100000\tepoch: 83, Avg Loss: 176.7600, Learning Rate: 0.00100000\tepoch: 84, Avg Loss: 193.6569, Learning Rate: 0.00100000\tepoch: 85, Avg Loss: 193.8495, Learning Rate: 0.00100000\tepoch: 86, Avg Loss: 185.6656, Learning Rate: 0.00100000\tepoch: 87, Avg Loss: 185.1404, Learning Rate: 0.00100000\tepoch: 88, Avg Loss: 196.2875, Learning Rate: 0.00100000\tepoch: 89, Avg Loss: 187.8067, Learning Rate: 0.00100000\tepoch: 90, Avg Loss: 167.9792, Learning Rate: 0.00100000\tepoch: 91, Avg Loss: 164.1864, Learning Rate: 0.00100000\tepoch: 92, Avg Loss: 162.9391, Learning Rate: 0.00100000\tepoch: 93, Avg Loss: 193.4648, Learning Rate: 0.00100000\tepoch: 94, Avg Loss: 161.0982, Learning Rate: 0.00100000\tepoch: 95, Avg Loss: 157.0602, Learning Rate: 0.00100000\tepoch: 96, Avg Loss: 145.0552, Learning Rate: 0.00100000\tepoch: 97, Avg Loss: 160.9237, Learning Rate: 0.00100000\tepoch: 98, Avg Loss: 184.4895, Learning Rate: 0.00100000\tepoch: 99, Avg Loss: 181.7775, Learning Rate: 0.00100000\tepoch: 100, Avg Loss: 144.7582, Learning Rate: 0.00100000\tepoch: 101, Avg Loss: 136.4743, Learning Rate: 0.00100000\tepoch: 102, Avg Loss: 137.6542, Learning Rate: 0.00100000\tepoch: 103, Avg Loss: 158.3595, Learning Rate: 0.00100000\tepoch: 104, Avg Loss: 138.6434, Learning Rate: 0.00100000\tepoch: 105, Avg Loss: 151.4526, Learning Rate: 0.00100000\tepoch: 106, Avg Loss: 158.0581, Learning Rate: 0.00100000\tepoch: 107, Avg Loss: 135.1715, Learning Rate: 0.00100000\tepoch: 108, Avg Loss: 130.8465, Learning Rate: 0.00100000\tepoch: 109, Avg Loss: 115.6789, Learning Rate: 0.00100000\tepoch: 110, Avg Loss: 131.2092, Learning Rate: 0.00100000\tepoch: 111, Avg Loss: 116.2899, Learning Rate: 0.00100000\tepoch: 112, Avg Loss: 129.0944, Learning Rate: 0.00100000\tepoch: 113, Avg Loss: 120.2790, Learning Rate: 0.00100000\tepoch: 114, Avg Loss: 122.7870, Learning Rate: 0.00100000\tepoch: 115, Avg Loss: 98.5452, Learning Rate: 0.00100000\tepoch: 116, Avg Loss: 109.1579, Learning Rate: 0.00100000\tepoch: 117, Avg Loss: 97.6515, Learning Rate: 0.00100000\tepoch: 118, Avg Loss: 112.3154, Learning Rate: 0.00100000\tepoch: 119, Avg Loss: 99.2569, Learning Rate: 0.00100000\tepoch: 120, Avg Loss: 98.7369, Learning Rate: 0.00100000\tepoch: 121, Avg Loss: 86.4389, Learning Rate: 0.00100000\tepoch: 122, Avg Loss: 81.0987, Learning Rate: 0.00100000\tepoch: 123, Avg Loss: 92.2343, Learning Rate: 0.00100000\tepoch: 124, Avg Loss: 94.7927, Learning Rate: 0.00100000\tepoch: 125, Avg Loss: 76.9323, Learning Rate: 0.00100000\tepoch: 126, Avg Loss: 75.3189, Learning Rate: 0.00100000\tepoch: 127, Avg Loss: 84.6467, Learning Rate: 0.00100000\tepoch: 128, Avg Loss: 71.4154, Learning Rate: 0.00100000\tepoch: 129, Avg Loss: 66.0688, Learning Rate: 0.00100000\tepoch: 130, Avg Loss: 63.7737, Learning Rate: 0.00100000\tepoch: 131, Avg Loss: 60.8659, Learning Rate: 0.00100000\tepoch: 132, Avg Loss: 66.8850, Learning Rate: 0.00100000\tepoch: 133, Avg Loss: 62.6216, Learning Rate: 0.00100000\tepoch: 134, Avg Loss: 52.0533, Learning Rate: 0.00100000\tepoch: 135, Avg Loss: 60.0671, Learning Rate: 0.00100000\tepoch: 136, Avg Loss: 60.0044, Learning Rate: 0.00100000\tepoch: 137, Avg Loss: 51.5637, Learning Rate: 0.00100000\tepoch: 138, Avg Loss: 54.9536, Learning Rate: 0.00100000\tepoch: 140, Avg Loss: 48.1159, Learning Rate: 0.00100000\tepoch: 141, Avg Loss: 42.1138, Learning Rate: 0.00100000\tepoch: 142, Avg Loss: 39.8769, Learning Rate: 0.00100000\tepoch: 143, Avg Loss: 36.3323, Learning Rate: 0.00100000\tepoch: 144, Avg Loss: 31.7623, Learning Rate: 0.00100000\tepoch: 145, Avg Loss: 35.4408, Learning Rate: 0.00100000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 146, Avg Loss: 30.0403, Learning Rate: 0.00100000\tepoch: 147, Avg Loss: 32.0142, Learning Rate: 0.00100000\tepoch: 148, Avg Loss: 33.3584, Learning Rate: 0.00100000\tepoch: 149, Avg Loss: 31.6506, Learning Rate: 0.00100000\tepoch: 150, Avg Loss: 26.3957, Learning Rate: 0.00100000\tepoch: 151, Avg Loss: 31.3200, Learning Rate: 0.00100000\tepoch: 152, Avg Loss: 26.1652, Learning Rate: 0.00100000\tepoch: 153, Avg Loss: 25.5295, Learning Rate: 0.00100000\tepoch: 154, Avg Loss: 21.4384, Learning Rate: 0.00100000\tepoch: 155, Avg Loss: 21.3700, Learning Rate: 0.00100000\tepoch: 156, Avg Loss: 19.6893, Learning Rate: 0.00100000\tepoch: 157, Avg Loss: 19.9119, Learning Rate: 0.00100000\tepoch: 158, Avg Loss: 21.4094, Learning Rate: 0.00100000\tepoch: 159, Avg Loss: 17.7508, Learning Rate: 0.00100000\tepoch: 160, Avg Loss: 17.5051, Learning Rate: 0.00100000\tepoch: 161, Avg Loss: 15.5936, Learning Rate: 0.00100000\tepoch: 162, Avg Loss: 15.7701, Learning Rate: 0.00100000\tepoch: 163, Avg Loss: 14.8141, Learning Rate: 0.00100000\tepoch: 164, Avg Loss: 14.1956, Learning Rate: 0.00100000\tepoch: 165, Avg Loss: 15.1221, Learning Rate: 0.00100000\tepoch: 166, Avg Loss: 15.6617, Learning Rate: 0.00100000\tepoch: 167, Avg Loss: 12.7116, Learning Rate: 0.00100000\tepoch: 168, Avg Loss: 12.5500, Learning Rate: 0.00100000\tepoch: 169, Avg Loss: 11.2918, Learning Rate: 0.00100000\tepoch: 170, Avg Loss: 11.0036, Learning Rate: 0.00100000\tepoch: 171, Avg Loss: 10.8971, Learning Rate: 0.00100000\tepoch: 172, Avg Loss: 11.8133, Learning Rate: 0.00100000\tepoch: 173, Avg Loss: 10.0085, Learning Rate: 0.00100000\tepoch: 174, Avg Loss: 8.4054, Learning Rate: 0.00100000\tepoch: 175, Avg Loss: 8.8818, Learning Rate: 0.00100000\tepoch: 176, Avg Loss: 9.7643, Learning Rate: 0.00100000\tepoch: 177, Avg Loss: 8.6914, Learning Rate: 0.00100000\tepoch: 178, Avg Loss: 8.6739, Learning Rate: 0.00100000\tepoch: 179, Avg Loss: 8.0337, Learning Rate: 0.00100000\tepoch: 180, Avg Loss: 7.4068, Learning Rate: 0.00100000\tepoch: 181, Avg Loss: 8.4576, Learning Rate: 0.00100000\tepoch: 182, Avg Loss: 7.5522, Learning Rate: 0.00100000\tepoch: 183, Avg Loss: 6.6584, Learning Rate: 0.00100000\tepoch: 184, Avg Loss: 6.7611, Learning Rate: 0.00100000\tepoch: 185, Avg Loss: 6.1344, Learning Rate: 0.00100000\tepoch: 186, Avg Loss: 7.1056, Learning Rate: 0.00100000\tepoch: 187, Avg Loss: 6.6082, Learning Rate: 0.00100000\tepoch: 188, Avg Loss: 5.8304, Learning Rate: 0.00100000\tepoch: 189, Avg Loss: 6.3169, Learning Rate: 0.00100000\tepoch: 190, Avg Loss: 7.2436, Learning Rate: 0.00100000\tepoch: 191, Avg Loss: 6.1123, Learning Rate: 0.00100000\tepoch: 192, Avg Loss: 6.4704, Learning Rate: 0.00100000\tepoch: 193, Avg Loss: 5.9771, Learning Rate: 0.00100000\tepoch: 194, Avg Loss: 6.0809, Learning Rate: 0.00100000\tepoch: 195, Avg Loss: 5.2845, Learning Rate: 0.00100000\tepoch: 196, Avg Loss: 4.9679, Learning Rate: 0.00100000\tepoch: 197, Avg Loss: 6.6028, Learning Rate: 0.00100000\tepoch: 198, Avg Loss: 5.5539, Learning Rate: 0.00100000\tepoch: 199, Avg Loss: 5.1752, Learning Rate: 0.00100000\tepoch: 200, Avg Loss: 5.3404, Learning Rate: 0.00100000\tepoch: 201, Avg Loss: 4.8118, Learning Rate: 0.00100000\tepoch: 202, Avg Loss: 6.0986, Learning Rate: 0.00100000\tepoch: 203, Avg Loss: 6.0862, Learning Rate: 0.00100000\tepoch: 204, Avg Loss: 4.9594, Learning Rate: 0.00100000\tepoch: 205, Avg Loss: 5.0922, Learning Rate: 0.00100000\tepoch: 206, Avg Loss: 4.8495, Learning Rate: 0.00100000\tepoch: 207, Avg Loss: 4.5234, Learning Rate: 0.00100000\tepoch: 208, Avg Loss: 5.4144, Learning Rate: 0.00100000\tepoch: 209, Avg Loss: 5.3338, Learning Rate: 0.00100000\tepoch: 210, Avg Loss: 4.4426, Learning Rate: 0.00100000\tepoch: 211, Avg Loss: 4.0182, Learning Rate: 0.00100000\tepoch: 212, Avg Loss: 3.9082, Learning Rate: 0.00100000\tepoch: 213, Avg Loss: 3.9730, Learning Rate: 0.00100000\tepoch: 214, Avg Loss: 4.0949, Learning Rate: 0.00100000\tepoch: 215, Avg Loss: 4.8116, Learning Rate: 0.00100000\tepoch: 216, Avg Loss: 5.1905, Learning Rate: 0.00100000\tepoch: 217, Avg Loss: 4.1318, Learning Rate: 0.00100000\tepoch: 218, Avg Loss: 4.0569, Learning Rate: 0.00100000\tepoch: 219, Avg Loss: 5.0028, Learning Rate: 0.00100000\tepoch: 220, Avg Loss: 4.6957, Learning Rate: 0.00100000\tepoch: 221, Avg Loss: 4.7312, Learning Rate: 0.00100000\tepoch: 222, Avg Loss: 4.6935, Learning Rate: 0.00100000\tepoch: 223, Avg Loss: 5.3676, Learning Rate: 0.00100000\tepoch: 224, Avg Loss: 4.1386, Learning Rate: 0.00100000\tepoch: 225, Avg Loss: 3.8116, Learning Rate: 0.00100000\tepoch: 226, Avg Loss: 4.4747, Learning Rate: 0.00100000\tepoch: 227, Avg Loss: 4.4150, Learning Rate: 0.00100000\tepoch: 228, Avg Loss: 3.8207, Learning Rate: 0.00100000\tepoch: 229, Avg Loss: 3.5729, Learning Rate: 0.00100000\tepoch: 230, Avg Loss: 4.6586, Learning Rate: 0.00100000\tepoch: 231, Avg Loss: 4.6401, Learning Rate: 0.00100000\tepoch: 232, Avg Loss: 3.9525, Learning Rate: 0.00100000\tepoch: 233, Avg Loss: 4.9203, Learning Rate: 0.00100000\tepoch: 234, Avg Loss: 4.5433, Learning Rate: 0.00100000\tepoch: 235, Avg Loss: 3.6582, Learning Rate: 0.00100000\tepoch: 236, Avg Loss: 3.3500, Learning Rate: 0.00100000\tepoch: 237, Avg Loss: 4.0174, Learning Rate: 0.00100000\tepoch: 238, Avg Loss: 4.1512, Learning Rate: 0.00100000\tepoch: 239, Avg Loss: 3.9822, Learning Rate: 0.00100000\tepoch: 240, Avg Loss: 4.5483, Learning Rate: 0.00100000\tepoch: 241, Avg Loss: 4.4338, Learning Rate: 0.00100000\tepoch: 242, Avg Loss: 3.9188, Learning Rate: 0.00100000\tepoch: 243, Avg Loss: 4.4005, Learning Rate: 0.00100000\tepoch: 244, Avg Loss: 4.3989, Learning Rate: 0.00100000\tepoch: 245, Avg Loss: 4.3804, Learning Rate: 0.00100000\tepoch: 246, Avg Loss: 3.3971, Learning Rate: 0.00100000\tepoch: 247, Avg Loss: 3.3481, Learning Rate: 0.00100000\tepoch: 248, Avg Loss: 4.3972, Learning Rate: 0.00100000\tepoch: 249, Avg Loss: 4.2832, Learning Rate: 0.00100000\tepoch: 250, Avg Loss: 3.2051, Learning Rate: 0.00100000\tepoch: 251, Avg Loss: 3.9077, Learning Rate: 0.00100000\tepoch: 252, Avg Loss: 3.5706, Learning Rate: 0.00100000\tepoch: 253, Avg Loss: 4.1732, Learning Rate: 0.00100000\tepoch: 254, Avg Loss: 3.6083, Learning Rate: 0.00100000\tepoch: 255, Avg Loss: 4.1430, Learning Rate: 0.00100000\tepoch: 256, Avg Loss: 4.3055, Learning Rate: 0.00100000\tepoch: 257, Avg Loss: 3.1692, Learning Rate: 0.00100000\tepoch: 258, Avg Loss: 3.5362, Learning Rate: 0.00100000\tepoch: 259, Avg Loss: 3.7271, Learning Rate: 0.00100000\tepoch: 260, Avg Loss: 4.2862, Learning Rate: 0.00100000\tepoch: 261, Avg Loss: 3.1548, Learning Rate: 0.00100000\tepoch: 262, Avg Loss: 3.6209, Learning Rate: 0.00100000\tepoch: 263, Avg Loss: 4.0955, Learning Rate: 0.00100000\tepoch: 264, Avg Loss: 3.1144, Learning Rate: 0.00100000\tepoch: 265, Avg Loss: 3.0957, Learning Rate: 0.00100000\tepoch: 266, Avg Loss: 4.1789, Learning Rate: 0.00100000\tepoch: 267, Avg Loss: 4.2036, Learning Rate: 0.00100000\tepoch: 268, Avg Loss: 4.1130, Learning Rate: 0.00100000\tepoch: 269, Avg Loss: 4.3193, Learning Rate: 0.00100000\tepoch: 270, Avg Loss: 4.1171, Learning Rate: 0.00100000\tepoch: 271, Avg Loss: 3.8448, Learning Rate: 0.00100000\tepoch: 272, Avg Loss: 3.8756, Learning Rate: 0.00100000\tepoch: 273, Avg Loss: 3.2234, Learning Rate: 0.00100000\tepoch: 274, Avg Loss: 3.5046, Learning Rate: 0.00100000\tepoch: 275, Avg Loss: 3.1658, Learning Rate: 0.00100000\tepoch: 276, Avg Loss: 4.1495, Learning Rate: 0.00100000\tepoch: 277, Avg Loss: 4.3516, Learning Rate: 0.00100000\tepoch: 278, Avg Loss: 3.3906, Learning Rate: 0.00100000\tepoch: 279, Avg Loss: 3.2808, Learning Rate: 0.00100000\tepoch: 280, Avg Loss: 3.4093, Learning Rate: 0.00100000\tepoch: 281, Avg Loss: 3.2668, Learning Rate: 0.00100000\tepoch: 282, Avg Loss: 3.6935, Learning Rate: 0.00100000\tepoch: 283, Avg Loss: 3.0940, Learning Rate: 0.00100000\tepoch: 284, Avg Loss: 3.1847, Learning Rate: 0.00100000\tepoch: 285, Avg Loss: 3.8453, Learning Rate: 0.00100000\tepoch: 286, Avg Loss: 3.4525, Learning Rate: 0.00100000\tepoch: 287, Avg Loss: 3.8171, Learning Rate: 0.00100000\tepoch: 288, Avg Loss: 3.6891, Learning Rate: 0.00100000\tepoch: 289, Avg Loss: 3.4711, Learning Rate: 0.00100000\tepoch: 290, Avg Loss: 3.0176, Learning Rate: 0.00100000\tepoch: 291, Avg Loss: 3.1960, Learning Rate: 0.00100000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 292, Avg Loss: 3.1342, Learning Rate: 0.00100000\tepoch: 293, Avg Loss: 2.9885, Learning Rate: 0.00100000\tepoch: 294, Avg Loss: 2.7978, Learning Rate: 0.00100000\tepoch: 295, Avg Loss: 4.4690, Learning Rate: 0.00100000\tepoch: 296, Avg Loss: 2.9928, Learning Rate: 0.00100000\tepoch: 297, Avg Loss: 2.9565, Learning Rate: 0.00100000\tepoch: 298, Avg Loss: 3.7185, Learning Rate: 0.00100000\tepoch: 299, Avg Loss: 3.2400, Learning Rate: 0.00100000\tepoch: 300, Avg Loss: 4.0102, Learning Rate: 0.00100000\tepoch: 301, Avg Loss: 2.9607, Learning Rate: 0.00100000\tepoch: 302, Avg Loss: 2.9875, Learning Rate: 0.00100000\tepoch: 303, Avg Loss: 3.8126, Learning Rate: 0.00100000\tepoch: 304, Avg Loss: 3.7449, Learning Rate: 0.00100000\tepoch: 305, Avg Loss: 4.2944, Learning Rate: 0.00100000\tepoch: 306, Avg Loss: 3.2754, Learning Rate: 0.00100000\tepoch: 307, Avg Loss: 4.3181, Learning Rate: 0.00100000\tepoch: 308, Avg Loss: 3.2631, Learning Rate: 0.00100000\tepoch: 309, Avg Loss: 3.2975, Learning Rate: 0.00100000\tepoch: 310, Avg Loss: 2.8876, Learning Rate: 0.00100000\tepoch: 311, Avg Loss: 3.6723, Learning Rate: 0.00100000\tepoch: 312, Avg Loss: 2.9066, Learning Rate: 0.00100000\tepoch: 313, Avg Loss: 3.7911, Learning Rate: 0.00100000\tepoch: 314, Avg Loss: 3.0570, Learning Rate: 0.00100000\tepoch: 315, Avg Loss: 4.1455, Learning Rate: 0.00100000\tepoch: 316, Avg Loss: 3.4791, Learning Rate: 0.00100000\tepoch: 317, Avg Loss: 3.0374, Learning Rate: 0.00100000\tepoch: 318, Avg Loss: 2.8594, Learning Rate: 0.00100000\tepoch: 319, Avg Loss: 3.1822, Learning Rate: 0.00100000\tepoch: 320, Avg Loss: 3.2380, Learning Rate: 0.00100000\tepoch: 321, Avg Loss: 3.9932, Learning Rate: 0.00100000\tepoch: 322, Avg Loss: 3.7064, Learning Rate: 0.00100000\tepoch: 323, Avg Loss: 3.1638, Learning Rate: 0.00100000\tepoch: 324, Avg Loss: 2.7856, Learning Rate: 0.00100000\tepoch: 325, Avg Loss: 4.0654, Learning Rate: 0.00100000\tepoch: 326, Avg Loss: 4.0057, Learning Rate: 0.00100000\tepoch: 327, Avg Loss: 3.4221, Learning Rate: 0.00100000\tepoch: 328, Avg Loss: 4.0977, Learning Rate: 0.00100000\tepoch: 329, Avg Loss: 3.8314, Learning Rate: 0.00100000\tepoch: 330, Avg Loss: 3.1360, Learning Rate: 0.00100000\tepoch: 331, Avg Loss: 2.9201, Learning Rate: 0.00100000\tepoch: 332, Avg Loss: 2.9886, Learning Rate: 0.00100000\tepoch: 333, Avg Loss: 2.8450, Learning Rate: 0.00100000\tepoch: 334, Avg Loss: 3.9079, Learning Rate: 0.00100000\tepoch: 335, Avg Loss: 3.2123, Learning Rate: 0.00100000\tepoch: 336, Avg Loss: 3.0503, Learning Rate: 0.00100000\tepoch: 337, Avg Loss: 2.8646, Learning Rate: 0.00100000\tepoch: 338, Avg Loss: 2.9631, Learning Rate: 0.00100000\tepoch: 339, Avg Loss: 3.0770, Learning Rate: 0.00100000\tepoch: 340, Avg Loss: 4.6069, Learning Rate: 0.00100000\tepoch: 341, Avg Loss: 3.5506, Learning Rate: 0.00100000\tepoch: 342, Avg Loss: 4.1609, Learning Rate: 0.00100000\tepoch: 343, Avg Loss: 3.8975, Learning Rate: 0.00100000\tepoch: 344, Avg Loss: 4.0157, Learning Rate: 0.00100000\tepoch: 345, Avg Loss: 4.0588, Learning Rate: 0.00100000\tepoch: 346, Avg Loss: 3.2889, Learning Rate: 0.00100000\tepoch: 347, Avg Loss: 3.1740, Learning Rate: 0.00100000\tepoch: 348, Avg Loss: 3.6328, Learning Rate: 0.00100000\tepoch: 349, Avg Loss: 3.3846, Learning Rate: 0.00100000\tepoch: 350, Avg Loss: 3.7971, Learning Rate: 0.00100000\tepoch: 351, Avg Loss: 2.8611, Learning Rate: 0.00100000\tepoch: 352, Avg Loss: 2.6735, Learning Rate: 0.00100000\tepoch: 353, Avg Loss: 4.0172, Learning Rate: 0.00100000\tepoch: 354, Avg Loss: 3.2819, Learning Rate: 0.00100000\tepoch: 355, Avg Loss: 4.3067, Learning Rate: 0.00100000\tepoch: 356, Avg Loss: 3.0791, Learning Rate: 0.00100000\tepoch: 357, Avg Loss: 3.2082, Learning Rate: 0.00100000\tepoch: 358, Avg Loss: 3.0078, Learning Rate: 0.00100000\tepoch: 359, Avg Loss: 3.4142, Learning Rate: 0.00100000\tepoch: 360, Avg Loss: 3.7164, Learning Rate: 0.00100000\tepoch: 361, Avg Loss: 3.1173, Learning Rate: 0.00100000\tepoch: 362, Avg Loss: 3.1312, Learning Rate: 0.00100000\tepoch: 363, Avg Loss: 2.8191, Learning Rate: 0.00100000\tepoch: 364, Avg Loss: 3.9783, Learning Rate: 0.00100000\tepoch: 365, Avg Loss: 3.4009, Learning Rate: 0.00100000\tepoch: 366, Avg Loss: 4.0160, Learning Rate: 0.00100000\tepoch: 367, Avg Loss: 2.9480, Learning Rate: 0.00100000\tepoch: 368, Avg Loss: 3.0611, Learning Rate: 0.00100000\tepoch: 369, Avg Loss: 3.0374, Learning Rate: 0.00100000\tepoch: 370, Avg Loss: 2.7128, Learning Rate: 0.00100000\tepoch: 371, Avg Loss: 3.0714, Learning Rate: 0.00100000\tepoch: 372, Avg Loss: 3.2073, Learning Rate: 0.00100000\tepoch: 373, Avg Loss: 3.2516, Learning Rate: 0.00100000\tepoch: 374, Avg Loss: 2.7853, Learning Rate: 0.00100000\tepoch: 375, Avg Loss: 2.5644, Learning Rate: 0.00100000\tepoch: 376, Avg Loss: 2.9990, Learning Rate: 0.00100000\tepoch: 377, Avg Loss: 3.6872, Learning Rate: 0.00100000\tepoch: 378, Avg Loss: 2.8381, Learning Rate: 0.00100000\tepoch: 379, Avg Loss: 3.7781, Learning Rate: 0.00100000\tepoch: 380, Avg Loss: 2.9302, Learning Rate: 0.00100000\tepoch: 381, Avg Loss: 3.3126, Learning Rate: 0.00100000\tepoch: 382, Avg Loss: 3.3754, Learning Rate: 0.00100000\tepoch: 383, Avg Loss: 2.8878, Learning Rate: 0.00100000\tepoch: 384, Avg Loss: 3.1475, Learning Rate: 0.00100000\tepoch: 385, Avg Loss: 3.8172, Learning Rate: 0.00100000\tepoch: 386, Avg Loss: 3.7457, Learning Rate: 0.00100000\tepoch: 387, Avg Loss: 3.4626, Learning Rate: 0.00100000\tepoch: 388, Avg Loss: 3.9396, Learning Rate: 0.00100000\tepoch: 389, Avg Loss: 3.1011, Learning Rate: 0.00100000\tepoch: 390, Avg Loss: 2.9850, Learning Rate: 0.00100000\tepoch: 391, Avg Loss: 2.9165, Learning Rate: 0.00100000\tepoch: 392, Avg Loss: 2.8960, Learning Rate: 0.00100000\tepoch: 393, Avg Loss: 3.1435, Learning Rate: 0.00100000\tepoch: 394, Avg Loss: 3.0442, Learning Rate: 0.00100000\tepoch: 395, Avg Loss: 3.2399, Learning Rate: 0.00100000\tepoch: 396, Avg Loss: 2.9276, Learning Rate: 0.00100000\tepoch: 397, Avg Loss: 3.0995, Learning Rate: 0.00100000\tepoch: 398, Avg Loss: 3.6305, Learning Rate: 0.00100000\tepoch: 399, Avg Loss: 3.7305, Learning Rate: 0.00100000\tepoch: 400, Avg Loss: 3.4927, Learning Rate: 0.00100000\tepoch: 401, Avg Loss: 3.4422, Learning Rate: 0.00100000\tepoch: 402, Avg Loss: 2.8362, Learning Rate: 0.00100000\tepoch: 403, Avg Loss: 3.1708, Learning Rate: 0.00100000\tepoch: 404, Avg Loss: 3.9764, Learning Rate: 0.00100000\tepoch: 405, Avg Loss: 3.7365, Learning Rate: 0.00100000\tepoch: 406, Avg Loss: 2.7947, Learning Rate: 0.00100000\tepoch: 407, Avg Loss: 2.7731, Learning Rate: 0.00100000\tepoch: 408, Avg Loss: 2.7862, Learning Rate: 0.00100000\tepoch: 409, Avg Loss: 3.6867, Learning Rate: 0.00100000\tepoch: 410, Avg Loss: 3.7740, Learning Rate: 0.00100000\tepoch: 411, Avg Loss: 3.3315, Learning Rate: 0.00100000\tepoch: 412, Avg Loss: 4.2087, Learning Rate: 0.00100000\tepoch: 413, Avg Loss: 2.9904, Learning Rate: 0.00100000\tepoch: 414, Avg Loss: 3.1064, Learning Rate: 0.00100000\tepoch: 415, Avg Loss: 3.0232, Learning Rate: 0.00100000\tepoch: 416, Avg Loss: 3.7268, Learning Rate: 0.00100000\tepoch: 417, Avg Loss: 2.8247, Learning Rate: 0.00100000\tepoch: 418, Avg Loss: 3.1650, Learning Rate: 0.00100000\tepoch: 419, Avg Loss: 2.8995, Learning Rate: 0.00100000\tepoch: 420, Avg Loss: 2.6914, Learning Rate: 0.00100000\tepoch: 421, Avg Loss: 3.1312, Learning Rate: 0.00100000\tepoch: 422, Avg Loss: 3.1306, Learning Rate: 0.00100000\tepoch: 423, Avg Loss: 3.5351, Learning Rate: 0.00100000\tepoch: 424, Avg Loss: 3.4951, Learning Rate: 0.00100000\tepoch: 425, Avg Loss: 3.0244, Learning Rate: 0.00100000\tepoch: 426, Avg Loss: 3.5886, Learning Rate: 0.00050000\tepoch: 427, Avg Loss: 2.9574, Learning Rate: 0.00050000\tepoch: 428, Avg Loss: 3.0153, Learning Rate: 0.00050000\tepoch: 429, Avg Loss: 4.0230, Learning Rate: 0.00050000\tepoch: 430, Avg Loss: 3.6170, Learning Rate: 0.00050000\tepoch: 431, Avg Loss: 2.5681, Learning Rate: 0.00050000\tepoch: 432, Avg Loss: 3.8240, Learning Rate: 0.00050000\tepoch: 433, Avg Loss: 3.2224, Learning Rate: 0.00050000\tepoch: 434, Avg Loss: 3.0798, Learning Rate: 0.00050000\tepoch: 435, Avg Loss: 2.7917, Learning Rate: 0.00050000\tepoch: 436, Avg Loss: 2.6665, Learning Rate: 0.00050000\tepoch: 437, Avg Loss: 3.4726, Learning Rate: 0.00050000\tepoch: 438, Avg Loss: 2.6175, Learning Rate: 0.00050000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 439, Avg Loss: 3.6382, Learning Rate: 0.00050000\tepoch: 440, Avg Loss: 3.1259, Learning Rate: 0.00050000\tepoch: 441, Avg Loss: 2.8843, Learning Rate: 0.00050000\tepoch: 442, Avg Loss: 2.6708, Learning Rate: 0.00050000\tepoch: 443, Avg Loss: 2.7593, Learning Rate: 0.00050000\tepoch: 444, Avg Loss: 2.9938, Learning Rate: 0.00050000\tepoch: 445, Avg Loss: 3.9957, Learning Rate: 0.00050000\tepoch: 446, Avg Loss: 3.9226, Learning Rate: 0.00050000\tepoch: 447, Avg Loss: 3.2252, Learning Rate: 0.00050000\tepoch: 448, Avg Loss: 2.8578, Learning Rate: 0.00050000\tepoch: 449, Avg Loss: 3.4436, Learning Rate: 0.00050000\tepoch: 450, Avg Loss: 3.5962, Learning Rate: 0.00050000\tepoch: 451, Avg Loss: 2.5698, Learning Rate: 0.00050000\tepoch: 452, Avg Loss: 3.4360, Learning Rate: 0.00050000\tepoch: 453, Avg Loss: 3.7251, Learning Rate: 0.00050000\tepoch: 454, Avg Loss: 3.3711, Learning Rate: 0.00050000\tepoch: 455, Avg Loss: 4.0636, Learning Rate: 0.00050000\tepoch: 456, Avg Loss: 4.1133, Learning Rate: 0.00050000\tepoch: 457, Avg Loss: 2.9779, Learning Rate: 0.00050000\tepoch: 458, Avg Loss: 3.7043, Learning Rate: 0.00050000\tepoch: 459, Avg Loss: 2.6577, Learning Rate: 0.00050000\tepoch: 460, Avg Loss: 3.6459, Learning Rate: 0.00050000\tepoch: 461, Avg Loss: 3.4342, Learning Rate: 0.00050000\tepoch: 462, Avg Loss: 2.5316, Learning Rate: 0.00050000\tepoch: 463, Avg Loss: 3.0278, Learning Rate: 0.00050000\tepoch: 464, Avg Loss: 4.1347, Learning Rate: 0.00050000\tepoch: 465, Avg Loss: 2.7157, Learning Rate: 0.00050000\tepoch: 466, Avg Loss: 2.9861, Learning Rate: 0.00050000\tepoch: 467, Avg Loss: 3.0907, Learning Rate: 0.00050000\tepoch: 468, Avg Loss: 2.5210, Learning Rate: 0.00050000\tepoch: 469, Avg Loss: 2.6786, Learning Rate: 0.00050000\tepoch: 470, Avg Loss: 2.7545, Learning Rate: 0.00050000\tepoch: 471, Avg Loss: 3.3357, Learning Rate: 0.00050000\tepoch: 472, Avg Loss: 3.6141, Learning Rate: 0.00050000\tepoch: 473, Avg Loss: 2.6381, Learning Rate: 0.00050000\tepoch: 474, Avg Loss: 3.6491, Learning Rate: 0.00050000\tepoch: 475, Avg Loss: 3.0652, Learning Rate: 0.00050000\tepoch: 476, Avg Loss: 2.9076, Learning Rate: 0.00050000\tepoch: 477, Avg Loss: 2.9348, Learning Rate: 0.00050000\tepoch: 478, Avg Loss: 2.5344, Learning Rate: 0.00050000\tepoch: 479, Avg Loss: 2.9105, Learning Rate: 0.00050000\tepoch: 480, Avg Loss: 2.9481, Learning Rate: 0.00050000\tepoch: 481, Avg Loss: 2.7005, Learning Rate: 0.00050000\tepoch: 482, Avg Loss: 2.6971, Learning Rate: 0.00050000\tepoch: 483, Avg Loss: 2.5193, Learning Rate: 0.00050000\tepoch: 484, Avg Loss: 2.8724, Learning Rate: 0.00050000\tepoch: 485, Avg Loss: 3.0131, Learning Rate: 0.00050000\tepoch: 486, Avg Loss: 2.8423, Learning Rate: 0.00050000\tepoch: 487, Avg Loss: 4.2480, Learning Rate: 0.00050000\tepoch: 488, Avg Loss: 2.8763, Learning Rate: 0.00050000\tepoch: 489, Avg Loss: 2.6338, Learning Rate: 0.00050000\tepoch: 490, Avg Loss: 3.8560, Learning Rate: 0.00050000\tepoch: 491, Avg Loss: 2.6803, Learning Rate: 0.00050000\tepoch: 492, Avg Loss: 2.7894, Learning Rate: 0.00050000\tepoch: 493, Avg Loss: 2.6526, Learning Rate: 0.00050000\tepoch: 494, Avg Loss: 2.9990, Learning Rate: 0.00050000\tepoch: 495, Avg Loss: 3.1322, Learning Rate: 0.00050000\tepoch: 496, Avg Loss: 3.9912, Learning Rate: 0.00050000\tepoch: 497, Avg Loss: 2.8777, Learning Rate: 0.00050000\tepoch: 498, Avg Loss: 3.8019, Learning Rate: 0.00050000\tepoch: 499, Avg Loss: 3.3273, Learning Rate: 0.00050000\tepoch: 500, Avg Loss: 3.3912, Learning Rate: 0.00050000\tepoch: 501, Avg Loss: 3.1236, Learning Rate: 0.00050000\tepoch: 502, Avg Loss: 3.0399, Learning Rate: 0.00050000\tepoch: 503, Avg Loss: 2.8462, Learning Rate: 0.00050000\tepoch: 504, Avg Loss: 2.8101, Learning Rate: 0.00050000\tepoch: 505, Avg Loss: 2.8238, Learning Rate: 0.00050000\tepoch: 506, Avg Loss: 4.1573, Learning Rate: 0.00050000\tepoch: 507, Avg Loss: 2.5700, Learning Rate: 0.00050000\tepoch: 508, Avg Loss: 2.5223, Learning Rate: 0.00050000\tepoch: 509, Avg Loss: 2.8642, Learning Rate: 0.00050000\tepoch: 510, Avg Loss: 4.0045, Learning Rate: 0.00050000\tepoch: 511, Avg Loss: 2.4367, Learning Rate: 0.00050000\tepoch: 512, Avg Loss: 3.6624, Learning Rate: 0.00050000\tepoch: 513, Avg Loss: 2.5585, Learning Rate: 0.00050000\tepoch: 514, Avg Loss: 2.5458, Learning Rate: 0.00050000\tepoch: 515, Avg Loss: 2.8719, Learning Rate: 0.00050000\tepoch: 516, Avg Loss: 3.0476, Learning Rate: 0.00050000\tepoch: 517, Avg Loss: 3.7001, Learning Rate: 0.00050000\tepoch: 518, Avg Loss: 2.6076, Learning Rate: 0.00050000\tepoch: 519, Avg Loss: 3.3446, Learning Rate: 0.00050000\tepoch: 520, Avg Loss: 2.6638, Learning Rate: 0.00050000\tepoch: 521, Avg Loss: 3.4398, Learning Rate: 0.00050000\tepoch: 522, Avg Loss: 3.8061, Learning Rate: 0.00050000\tepoch: 523, Avg Loss: 2.8240, Learning Rate: 0.00050000\tepoch: 524, Avg Loss: 3.8131, Learning Rate: 0.00050000\tepoch: 525, Avg Loss: 2.9077, Learning Rate: 0.00050000\tepoch: 526, Avg Loss: 2.6541, Learning Rate: 0.00050000\tepoch: 527, Avg Loss: 2.6796, Learning Rate: 0.00050000\tepoch: 528, Avg Loss: 2.4201, Learning Rate: 0.00050000\tepoch: 529, Avg Loss: 2.5331, Learning Rate: 0.00050000\tepoch: 530, Avg Loss: 2.5857, Learning Rate: 0.00050000\tepoch: 531, Avg Loss: 2.9572, Learning Rate: 0.00050000\tepoch: 532, Avg Loss: 3.6788, Learning Rate: 0.00050000\tepoch: 533, Avg Loss: 3.5666, Learning Rate: 0.00050000\tepoch: 534, Avg Loss: 2.5600, Learning Rate: 0.00050000\tepoch: 535, Avg Loss: 2.5749, Learning Rate: 0.00050000\tepoch: 536, Avg Loss: 3.7329, Learning Rate: 0.00050000\tepoch: 537, Avg Loss: 2.6606, Learning Rate: 0.00050000\tepoch: 538, Avg Loss: 3.3742, Learning Rate: 0.00050000\tepoch: 539, Avg Loss: 2.6641, Learning Rate: 0.00050000\tepoch: 540, Avg Loss: 2.9405, Learning Rate: 0.00050000\tepoch: 541, Avg Loss: 3.6235, Learning Rate: 0.00050000\tepoch: 542, Avg Loss: 2.8062, Learning Rate: 0.00050000\tepoch: 543, Avg Loss: 2.8972, Learning Rate: 0.00050000\tepoch: 544, Avg Loss: 4.1566, Learning Rate: 0.00050000\tepoch: 545, Avg Loss: 2.5386, Learning Rate: 0.00050000\tepoch: 546, Avg Loss: 3.3456, Learning Rate: 0.00050000\tepoch: 547, Avg Loss: 2.7439, Learning Rate: 0.00050000\tepoch: 548, Avg Loss: 2.8168, Learning Rate: 0.00050000\tepoch: 549, Avg Loss: 3.5286, Learning Rate: 0.00050000\tepoch: 550, Avg Loss: 2.7964, Learning Rate: 0.00050000\tepoch: 551, Avg Loss: 2.5552, Learning Rate: 0.00050000\tepoch: 552, Avg Loss: 2.9680, Learning Rate: 0.00050000\tepoch: 553, Avg Loss: 2.8212, Learning Rate: 0.00050000\tepoch: 554, Avg Loss: 2.9572, Learning Rate: 0.00050000\tepoch: 555, Avg Loss: 2.8365, Learning Rate: 0.00050000\tepoch: 556, Avg Loss: 2.7232, Learning Rate: 0.00050000\tepoch: 557, Avg Loss: 2.8897, Learning Rate: 0.00050000\tepoch: 558, Avg Loss: 2.6789, Learning Rate: 0.00050000\tepoch: 559, Avg Loss: 2.6210, Learning Rate: 0.00050000\tepoch: 560, Avg Loss: 3.1011, Learning Rate: 0.00050000\tepoch: 561, Avg Loss: 3.0234, Learning Rate: 0.00050000\tepoch: 562, Avg Loss: 2.9594, Learning Rate: 0.00050000\tepoch: 563, Avg Loss: 2.6946, Learning Rate: 0.00050000\tepoch: 564, Avg Loss: 2.9779, Learning Rate: 0.00050000\tepoch: 565, Avg Loss: 2.5405, Learning Rate: 0.00050000\tepoch: 566, Avg Loss: 2.8880, Learning Rate: 0.00050000\tepoch: 567, Avg Loss: 3.1225, Learning Rate: 0.00050000\tepoch: 568, Avg Loss: 3.6792, Learning Rate: 0.00050000\tepoch: 569, Avg Loss: 3.3626, Learning Rate: 0.00050000\tepoch: 570, Avg Loss: 3.5661, Learning Rate: 0.00050000\tepoch: 571, Avg Loss: 2.9163, Learning Rate: 0.00050000\tepoch: 572, Avg Loss: 2.7215, Learning Rate: 0.00050000\tepoch: 573, Avg Loss: 2.9129, Learning Rate: 0.00050000\tepoch: 574, Avg Loss: 2.7856, Learning Rate: 0.00050000\tepoch: 575, Avg Loss: 2.7806, Learning Rate: 0.00050000\tepoch: 576, Avg Loss: 2.8140, Learning Rate: 0.00050000\tepoch: 577, Avg Loss: 3.3743, Learning Rate: 0.00050000\tepoch: 578, Avg Loss: 2.4551, Learning Rate: 0.00050000\tepoch: 579, Avg Loss: 3.1740, Learning Rate: 0.00025000\tepoch: 580, Avg Loss: 3.8630, Learning Rate: 0.00025000\tepoch: 581, Avg Loss: 2.9230, Learning Rate: 0.00025000\tepoch: 582, Avg Loss: 3.2139, Learning Rate: 0.00025000\tepoch: 583, Avg Loss: 3.5640, Learning Rate: 0.00025000\tepoch: 584, Avg Loss: 2.4321, Learning Rate: 0.00025000\tepoch: 585, Avg Loss: 3.6553, Learning Rate: 0.00025000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 586, Avg Loss: 3.4293, Learning Rate: 0.00025000\tepoch: 587, Avg Loss: 3.5634, Learning Rate: 0.00025000\tepoch: 588, Avg Loss: 2.4788, Learning Rate: 0.00025000\tepoch: 589, Avg Loss: 2.9642, Learning Rate: 0.00025000\tepoch: 590, Avg Loss: 3.5723, Learning Rate: 0.00025000\tepoch: 591, Avg Loss: 3.4837, Learning Rate: 0.00025000\tepoch: 592, Avg Loss: 3.4830, Learning Rate: 0.00025000\tepoch: 593, Avg Loss: 2.5772, Learning Rate: 0.00025000\tepoch: 594, Avg Loss: 3.7266, Learning Rate: 0.00025000\tepoch: 595, Avg Loss: 2.8498, Learning Rate: 0.00025000\tepoch: 596, Avg Loss: 2.7341, Learning Rate: 0.00025000\tepoch: 597, Avg Loss: 2.5262, Learning Rate: 0.00025000\tepoch: 598, Avg Loss: 3.3290, Learning Rate: 0.00025000\tepoch: 599, Avg Loss: 2.4552, Learning Rate: 0.00025000\tepoch: 600, Avg Loss: 2.6198, Learning Rate: 0.00025000\tepoch: 601, Avg Loss: 2.4639, Learning Rate: 0.00025000\tepoch: 602, Avg Loss: 3.7754, Learning Rate: 0.00025000\tepoch: 603, Avg Loss: 2.8609, Learning Rate: 0.00025000\tepoch: 604, Avg Loss: 2.6228, Learning Rate: 0.00025000\tepoch: 605, Avg Loss: 2.4439, Learning Rate: 0.00025000\tepoch: 606, Avg Loss: 3.6187, Learning Rate: 0.00025000\tepoch: 607, Avg Loss: 3.6340, Learning Rate: 0.00025000\tepoch: 608, Avg Loss: 3.1873, Learning Rate: 0.00025000\tepoch: 609, Avg Loss: 2.7307, Learning Rate: 0.00025000\tepoch: 610, Avg Loss: 3.7493, Learning Rate: 0.00025000\tepoch: 611, Avg Loss: 3.0006, Learning Rate: 0.00025000\tepoch: 612, Avg Loss: 2.4803, Learning Rate: 0.00025000\tepoch: 613, Avg Loss: 2.8662, Learning Rate: 0.00025000\tepoch: 614, Avg Loss: 2.9822, Learning Rate: 0.00025000\tepoch: 615, Avg Loss: 2.9474, Learning Rate: 0.00025000\tepoch: 616, Avg Loss: 2.5426, Learning Rate: 0.00025000\tepoch: 617, Avg Loss: 2.9573, Learning Rate: 0.00025000\tepoch: 618, Avg Loss: 3.1604, Learning Rate: 0.00025000\tepoch: 619, Avg Loss: 3.0649, Learning Rate: 0.00025000\tepoch: 620, Avg Loss: 2.7162, Learning Rate: 0.00025000\tepoch: 621, Avg Loss: 3.4555, Learning Rate: 0.00025000\tepoch: 622, Avg Loss: 3.2675, Learning Rate: 0.00025000\tepoch: 623, Avg Loss: 3.1852, Learning Rate: 0.00025000\tepoch: 624, Avg Loss: 3.0509, Learning Rate: 0.00025000\tepoch: 625, Avg Loss: 2.4742, Learning Rate: 0.00025000\tepoch: 626, Avg Loss: 2.3955, Learning Rate: 0.00025000\tepoch: 627, Avg Loss: 2.5585, Learning Rate: 0.00025000\tepoch: 628, Avg Loss: 3.3356, Learning Rate: 0.00025000\tepoch: 629, Avg Loss: 2.6294, Learning Rate: 0.00025000\tepoch: 630, Avg Loss: 3.6074, Learning Rate: 0.00025000\tepoch: 631, Avg Loss: 2.7745, Learning Rate: 0.00025000\tepoch: 632, Avg Loss: 2.8310, Learning Rate: 0.00025000\tepoch: 633, Avg Loss: 3.2056, Learning Rate: 0.00025000\tepoch: 634, Avg Loss: 3.1862, Learning Rate: 0.00025000\tepoch: 635, Avg Loss: 3.7842, Learning Rate: 0.00025000\tepoch: 636, Avg Loss: 3.3642, Learning Rate: 0.00025000\tepoch: 637, Avg Loss: 2.6227, Learning Rate: 0.00025000\tepoch: 638, Avg Loss: 2.9037, Learning Rate: 0.00025000\tepoch: 639, Avg Loss: 2.7048, Learning Rate: 0.00025000\tepoch: 640, Avg Loss: 2.5050, Learning Rate: 0.00025000\tepoch: 641, Avg Loss: 3.1622, Learning Rate: 0.00025000\tepoch: 642, Avg Loss: 2.7711, Learning Rate: 0.00025000\tepoch: 643, Avg Loss: 2.5020, Learning Rate: 0.00025000\tepoch: 644, Avg Loss: 3.5331, Learning Rate: 0.00025000\tepoch: 645, Avg Loss: 2.7470, Learning Rate: 0.00025000\tepoch: 646, Avg Loss: 3.5583, Learning Rate: 0.00025000\tepoch: 647, Avg Loss: 2.4965, Learning Rate: 0.00025000\tepoch: 648, Avg Loss: 3.6647, Learning Rate: 0.00025000\tepoch: 649, Avg Loss: 2.7209, Learning Rate: 0.00025000\tepoch: 650, Avg Loss: 2.4673, Learning Rate: 0.00025000\tepoch: 651, Avg Loss: 3.8534, Learning Rate: 0.00025000\tepoch: 652, Avg Loss: 2.8911, Learning Rate: 0.00025000\tepoch: 653, Avg Loss: 3.4863, Learning Rate: 0.00025000\tepoch: 654, Avg Loss: 2.7609, Learning Rate: 0.00025000\tepoch: 655, Avg Loss: 2.7024, Learning Rate: 0.00025000\tepoch: 656, Avg Loss: 2.5505, Learning Rate: 0.00025000\tepoch: 657, Avg Loss: 2.3481, Learning Rate: 0.00025000\tepoch: 658, Avg Loss: 2.4856, Learning Rate: 0.00025000\tepoch: 659, Avg Loss: 2.3883, Learning Rate: 0.00025000\tepoch: 660, Avg Loss: 2.6742, Learning Rate: 0.00025000\tepoch: 661, Avg Loss: 2.8647, Learning Rate: 0.00025000\tepoch: 662, Avg Loss: 2.6889, Learning Rate: 0.00025000\tepoch: 663, Avg Loss: 3.2308, Learning Rate: 0.00025000\tepoch: 664, Avg Loss: 2.4934, Learning Rate: 0.00025000\tepoch: 665, Avg Loss: 2.9279, Learning Rate: 0.00025000\tepoch: 666, Avg Loss: 3.3120, Learning Rate: 0.00025000\tepoch: 667, Avg Loss: 3.0047, Learning Rate: 0.00025000\tepoch: 668, Avg Loss: 2.4928, Learning Rate: 0.00025000\tepoch: 669, Avg Loss: 3.0536, Learning Rate: 0.00025000\tepoch: 670, Avg Loss: 3.4919, Learning Rate: 0.00025000\tepoch: 671, Avg Loss: 2.8174, Learning Rate: 0.00025000\tepoch: 672, Avg Loss: 2.7734, Learning Rate: 0.00025000\tepoch: 673, Avg Loss: 2.3926, Learning Rate: 0.00025000\tepoch: 674, Avg Loss: 2.4108, Learning Rate: 0.00025000\tepoch: 675, Avg Loss: 2.8945, Learning Rate: 0.00025000\tepoch: 676, Avg Loss: 2.8253, Learning Rate: 0.00025000\tepoch: 677, Avg Loss: 2.6449, Learning Rate: 0.00025000\tepoch: 678, Avg Loss: 3.1874, Learning Rate: 0.00025000\tepoch: 679, Avg Loss: 4.0397, Learning Rate: 0.00025000\tepoch: 680, Avg Loss: 3.5132, Learning Rate: 0.00025000\tepoch: 681, Avg Loss: 3.0052, Learning Rate: 0.00025000\tepoch: 682, Avg Loss: 2.6815, Learning Rate: 0.00025000\tepoch: 683, Avg Loss: 3.8902, Learning Rate: 0.00025000\tepoch: 684, Avg Loss: 3.1745, Learning Rate: 0.00025000\tepoch: 685, Avg Loss: 3.6496, Learning Rate: 0.00025000\tepoch: 686, Avg Loss: 2.7716, Learning Rate: 0.00025000\tepoch: 687, Avg Loss: 3.4452, Learning Rate: 0.00025000\tepoch: 688, Avg Loss: 2.6460, Learning Rate: 0.00025000\tepoch: 689, Avg Loss: 2.5890, Learning Rate: 0.00025000\tepoch: 690, Avg Loss: 2.4934, Learning Rate: 0.00025000\tepoch: 691, Avg Loss: 3.6299, Learning Rate: 0.00025000\tepoch: 692, Avg Loss: 2.8399, Learning Rate: 0.00025000\tepoch: 693, Avg Loss: 3.2287, Learning Rate: 0.00025000\tepoch: 694, Avg Loss: 3.4507, Learning Rate: 0.00025000\tepoch: 695, Avg Loss: 2.8883, Learning Rate: 0.00025000\tepoch: 696, Avg Loss: 2.6710, Learning Rate: 0.00025000\tepoch: 697, Avg Loss: 2.9307, Learning Rate: 0.00025000\tepoch: 698, Avg Loss: 3.7310, Learning Rate: 0.00025000\tepoch: 699, Avg Loss: 2.3703, Learning Rate: 0.00025000\tepoch: 700, Avg Loss: 3.5069, Learning Rate: 0.00025000\tepoch: 701, Avg Loss: 3.2201, Learning Rate: 0.00025000\tepoch: 702, Avg Loss: 3.2648, Learning Rate: 0.00025000\tepoch: 703, Avg Loss: 2.5261, Learning Rate: 0.00025000\tepoch: 704, Avg Loss: 2.6648, Learning Rate: 0.00025000\tepoch: 705, Avg Loss: 2.7469, Learning Rate: 0.00025000\tepoch: 706, Avg Loss: 2.5894, Learning Rate: 0.00025000\tepoch: 707, Avg Loss: 3.2647, Learning Rate: 0.00025000\tepoch: 708, Avg Loss: 2.3664, Learning Rate: 0.00012500\tepoch: 709, Avg Loss: 3.2182, Learning Rate: 0.00012500\tepoch: 710, Avg Loss: 2.4713, Learning Rate: 0.00012500\tepoch: 711, Avg Loss: 3.7246, Learning Rate: 0.00012500\tepoch: 712, Avg Loss: 3.1090, Learning Rate: 0.00012500\tepoch: 713, Avg Loss: 2.7808, Learning Rate: 0.00012500\tepoch: 714, Avg Loss: 3.5821, Learning Rate: 0.00012500\tepoch: 715, Avg Loss: 3.2549, Learning Rate: 0.00012500\tepoch: 716, Avg Loss: 2.4548, Learning Rate: 0.00012500\tepoch: 717, Avg Loss: 2.4131, Learning Rate: 0.00012500\tepoch: 718, Avg Loss: 2.7625, Learning Rate: 0.00012500\tepoch: 719, Avg Loss: 2.3688, Learning Rate: 0.00012500\tepoch: 720, Avg Loss: 2.4523, Learning Rate: 0.00012500\tepoch: 721, Avg Loss: 2.6775, Learning Rate: 0.00012500\tepoch: 722, Avg Loss: 3.4588, Learning Rate: 0.00012500\tepoch: 723, Avg Loss: 2.5704, Learning Rate: 0.00012500\tepoch: 724, Avg Loss: 3.2135, Learning Rate: 0.00012500\tepoch: 725, Avg Loss: 2.8093, Learning Rate: 0.00012500\tepoch: 726, Avg Loss: 3.2876, Learning Rate: 0.00012500\tepoch: 727, Avg Loss: 3.3773, Learning Rate: 0.00012500\tepoch: 728, Avg Loss: 2.8365, Learning Rate: 0.00012500\tepoch: 729, Avg Loss: 2.4708, Learning Rate: 0.00012500\tepoch: 730, Avg Loss: 2.7928, Learning Rate: 0.00012500\tepoch: 731, Avg Loss: 3.5536, Learning Rate: 0.00012500\tepoch: 732, Avg Loss: 3.8430, Learning Rate: 0.00012500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 733, Avg Loss: 2.8793, Learning Rate: 0.00012500\tepoch: 734, Avg Loss: 3.1863, Learning Rate: 0.00012500\tepoch: 735, Avg Loss: 2.7448, Learning Rate: 0.00012500\tepoch: 736, Avg Loss: 2.9619, Learning Rate: 0.00012500\tepoch: 737, Avg Loss: 2.6572, Learning Rate: 0.00012500\tepoch: 738, Avg Loss: 2.6171, Learning Rate: 0.00012500\tepoch: 739, Avg Loss: 3.1466, Learning Rate: 0.00012500\tepoch: 740, Avg Loss: 2.5640, Learning Rate: 0.00012500\tepoch: 741, Avg Loss: 2.9961, Learning Rate: 0.00012500\tepoch: 742, Avg Loss: 3.6786, Learning Rate: 0.00012500\tepoch: 743, Avg Loss: 2.5367, Learning Rate: 0.00012500\tepoch: 744, Avg Loss: 3.4548, Learning Rate: 0.00012500\tepoch: 745, Avg Loss: 2.4219, Learning Rate: 0.00012500\tepoch: 746, Avg Loss: 2.5692, Learning Rate: 0.00012500\tepoch: 747, Avg Loss: 2.7953, Learning Rate: 0.00012500\tepoch: 748, Avg Loss: 2.6818, Learning Rate: 0.00012500\tepoch: 749, Avg Loss: 2.6406, Learning Rate: 0.00012500\tepoch: 750, Avg Loss: 3.0046, Learning Rate: 0.00012500\tepoch: 751, Avg Loss: 3.1823, Learning Rate: 0.00012500\tepoch: 752, Avg Loss: 2.6996, Learning Rate: 0.00012500\tepoch: 753, Avg Loss: 2.7591, Learning Rate: 0.00012500\tepoch: 754, Avg Loss: 2.6917, Learning Rate: 0.00012500\tepoch: 755, Avg Loss: 2.7836, Learning Rate: 0.00012500\tepoch: 756, Avg Loss: 2.9775, Learning Rate: 0.00012500\tepoch: 757, Avg Loss: 2.7183, Learning Rate: 0.00012500\tepoch: 758, Avg Loss: 2.6902, Learning Rate: 0.00012500\tepoch: 759, Avg Loss: 3.1671, Learning Rate: 0.00006250\tepoch: 760, Avg Loss: 3.6648, Learning Rate: 0.00006250\tepoch: 761, Avg Loss: 3.7556, Learning Rate: 0.00006250\tepoch: 762, Avg Loss: 3.5721, Learning Rate: 0.00006250\tepoch: 763, Avg Loss: 3.6633, Learning Rate: 0.00006250\tepoch: 764, Avg Loss: 2.8749, Learning Rate: 0.00006250\tepoch: 765, Avg Loss: 2.5250, Learning Rate: 0.00006250\tepoch: 766, Avg Loss: 2.9315, Learning Rate: 0.00006250\tepoch: 767, Avg Loss: 2.5731, Learning Rate: 0.00006250\tepoch: 768, Avg Loss: 2.9142, Learning Rate: 0.00006250\tepoch: 769, Avg Loss: 2.8258, Learning Rate: 0.00006250\tepoch: 770, Avg Loss: 2.6890, Learning Rate: 0.00006250\tepoch: 771, Avg Loss: 3.7955, Learning Rate: 0.00006250\tepoch: 772, Avg Loss: 3.0048, Learning Rate: 0.00006250\tepoch: 773, Avg Loss: 2.4479, Learning Rate: 0.00006250\tepoch: 774, Avg Loss: 2.5521, Learning Rate: 0.00006250\tepoch: 775, Avg Loss: 3.2283, Learning Rate: 0.00006250\tepoch: 776, Avg Loss: 2.7528, Learning Rate: 0.00006250\tepoch: 777, Avg Loss: 3.1607, Learning Rate: 0.00006250\tepoch: 778, Avg Loss: 3.4569, Learning Rate: 0.00006250\tepoch: 779, Avg Loss: 2.9310, Learning Rate: 0.00006250\tepoch: 780, Avg Loss: 3.0071, Learning Rate: 0.00006250\tepoch: 781, Avg Loss: 2.6023, Learning Rate: 0.00006250\tepoch: 782, Avg Loss: 3.0436, Learning Rate: 0.00006250\tepoch: 783, Avg Loss: 2.6761, Learning Rate: 0.00006250\tepoch: 784, Avg Loss: 2.4109, Learning Rate: 0.00006250\tepoch: 785, Avg Loss: 2.5057, Learning Rate: 0.00006250\tepoch: 786, Avg Loss: 2.5683, Learning Rate: 0.00006250\tepoch: 787, Avg Loss: 3.4747, Learning Rate: 0.00006250\tepoch: 788, Avg Loss: 2.9701, Learning Rate: 0.00006250\tepoch: 789, Avg Loss: 2.4527, Learning Rate: 0.00006250\tepoch: 790, Avg Loss: 2.7222, Learning Rate: 0.00006250\tepoch: 791, Avg Loss: 2.6361, Learning Rate: 0.00006250\tepoch: 792, Avg Loss: 2.5773, Learning Rate: 0.00006250\tepoch: 793, Avg Loss: 2.8313, Learning Rate: 0.00006250\tepoch: 794, Avg Loss: 3.6258, Learning Rate: 0.00006250\tepoch: 795, Avg Loss: 2.3083, Learning Rate: 0.00006250\tepoch: 796, Avg Loss: 2.7677, Learning Rate: 0.00006250\tepoch: 797, Avg Loss: 3.4210, Learning Rate: 0.00006250\tepoch: 798, Avg Loss: 2.7248, Learning Rate: 0.00006250\tepoch: 799, Avg Loss: 3.4224, Learning Rate: 0.00006250\tepoch: 800, Avg Loss: 2.8554, Learning Rate: 0.00006250\tepoch: 801, Avg Loss: 3.3734, Learning Rate: 0.00006250\tepoch: 802, Avg Loss: 2.3411, Learning Rate: 0.00006250\tepoch: 803, Avg Loss: 3.8710, Learning Rate: 0.00006250\tepoch: 804, Avg Loss: 2.4997, Learning Rate: 0.00006250\tepoch: 805, Avg Loss: 2.4956, Learning Rate: 0.00006250\tepoch: 806, Avg Loss: 3.0157, Learning Rate: 0.00006250\tepoch: 807, Avg Loss: 2.8983, Learning Rate: 0.00006250\tepoch: 808, Avg Loss: 2.6174, Learning Rate: 0.00006250\tepoch: 809, Avg Loss: 2.8519, Learning Rate: 0.00006250\tepoch: 810, Avg Loss: 2.7720, Learning Rate: 0.00006250\tepoch: 811, Avg Loss: 3.1171, Learning Rate: 0.00006250\tepoch: 812, Avg Loss: 2.5115, Learning Rate: 0.00006250\tepoch: 813, Avg Loss: 2.5556, Learning Rate: 0.00006250\tepoch: 814, Avg Loss: 3.1276, Learning Rate: 0.00006250\tepoch: 815, Avg Loss: 2.7252, Learning Rate: 0.00006250\tepoch: 816, Avg Loss: 2.9114, Learning Rate: 0.00006250\tepoch: 817, Avg Loss: 2.3329, Learning Rate: 0.00006250\tepoch: 818, Avg Loss: 2.7476, Learning Rate: 0.00006250\tepoch: 819, Avg Loss: 3.4205, Learning Rate: 0.00006250\tepoch: 820, Avg Loss: 2.7910, Learning Rate: 0.00006250\tepoch: 821, Avg Loss: 2.8151, Learning Rate: 0.00006250\tepoch: 822, Avg Loss: 3.0604, Learning Rate: 0.00006250\tepoch: 823, Avg Loss: 3.2111, Learning Rate: 0.00006250\tepoch: 824, Avg Loss: 2.3994, Learning Rate: 0.00006250\tepoch: 825, Avg Loss: 2.6452, Learning Rate: 0.00006250\tepoch: 826, Avg Loss: 2.5069, Learning Rate: 0.00006250\tepoch: 827, Avg Loss: 2.5308, Learning Rate: 0.00006250\tepoch: 828, Avg Loss: 2.4135, Learning Rate: 0.00006250\tepoch: 829, Avg Loss: 3.6618, Learning Rate: 0.00006250\tepoch: 830, Avg Loss: 3.7946, Learning Rate: 0.00006250\tepoch: 831, Avg Loss: 3.3332, Learning Rate: 0.00006250\tepoch: 832, Avg Loss: 3.0606, Learning Rate: 0.00006250\tepoch: 833, Avg Loss: 2.7704, Learning Rate: 0.00006250\tepoch: 834, Avg Loss: 2.4736, Learning Rate: 0.00006250\tepoch: 835, Avg Loss: 2.3112, Learning Rate: 0.00006250\tepoch: 836, Avg Loss: 2.6835, Learning Rate: 0.00006250\tepoch: 837, Avg Loss: 2.3176, Learning Rate: 0.00006250\tepoch: 838, Avg Loss: 3.1106, Learning Rate: 0.00006250\tepoch: 839, Avg Loss: 2.7852, Learning Rate: 0.00006250\tepoch: 840, Avg Loss: 3.2476, Learning Rate: 0.00006250\tepoch: 841, Avg Loss: 3.0430, Learning Rate: 0.00006250\tepoch: 842, Avg Loss: 2.5695, Learning Rate: 0.00006250\tepoch: 843, Avg Loss: 3.6978, Learning Rate: 0.00006250\tepoch: 844, Avg Loss: 2.6694, Learning Rate: 0.00006250\tepoch: 845, Avg Loss: 3.3881, Learning Rate: 0.00006250\tepoch: 846, Avg Loss: 3.0368, Learning Rate: 0.00003125\tepoch: 847, Avg Loss: 2.7082, Learning Rate: 0.00003125\tepoch: 848, Avg Loss: 2.5946, Learning Rate: 0.00003125\tepoch: 849, Avg Loss: 3.4185, Learning Rate: 0.00003125\tepoch: 850, Avg Loss: 2.3177, Learning Rate: 0.00003125\tepoch: 851, Avg Loss: 2.4125, Learning Rate: 0.00003125\tepoch: 852, Avg Loss: 2.7186, Learning Rate: 0.00003125\tepoch: 853, Avg Loss: 3.2834, Learning Rate: 0.00003125\tepoch: 854, Avg Loss: 2.7923, Learning Rate: 0.00003125\tepoch: 855, Avg Loss: 2.3061, Learning Rate: 0.00003125\tepoch: 856, Avg Loss: 3.6782, Learning Rate: 0.00003125\tepoch: 857, Avg Loss: 3.5188, Learning Rate: 0.00003125\tepoch: 858, Avg Loss: 3.7833, Learning Rate: 0.00003125\tepoch: 859, Avg Loss: 2.4868, Learning Rate: 0.00003125\tepoch: 860, Avg Loss: 2.8390, Learning Rate: 0.00003125\tepoch: 861, Avg Loss: 2.3486, Learning Rate: 0.00003125\tepoch: 862, Avg Loss: 2.6176, Learning Rate: 0.00003125\tepoch: 863, Avg Loss: 3.1418, Learning Rate: 0.00003125\tepoch: 864, Avg Loss: 2.2729, Learning Rate: 0.00003125\tepoch: 865, Avg Loss: 2.6415, Learning Rate: 0.00003125\tepoch: 866, Avg Loss: 2.9124, Learning Rate: 0.00003125\tepoch: 867, Avg Loss: 2.4207, Learning Rate: 0.00003125\tepoch: 868, Avg Loss: 3.3840, Learning Rate: 0.00003125\tepoch: 869, Avg Loss: 2.4209, Learning Rate: 0.00003125\tepoch: 870, Avg Loss: 2.8897, Learning Rate: 0.00003125\tepoch: 871, Avg Loss: 2.4332, Learning Rate: 0.00003125\tepoch: 872, Avg Loss: 2.8676, Learning Rate: 0.00003125\tepoch: 873, Avg Loss: 2.3724, Learning Rate: 0.00003125\tepoch: 874, Avg Loss: 2.4331, Learning Rate: 0.00003125\tepoch: 875, Avg Loss: 3.6374, Learning Rate: 0.00003125\tepoch: 876, Avg Loss: 3.5112, Learning Rate: 0.00003125\tepoch: 877, Avg Loss: 2.7315, Learning Rate: 0.00003125\tepoch: 878, Avg Loss: 2.8280, Learning Rate: 0.00003125\tepoch: 879, Avg Loss: 2.7029, Learning Rate: 0.00003125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\tepoch: 880, Avg Loss: 2.7725, Learning Rate: 0.00003125\tepoch: 881, Avg Loss: 2.7230, Learning Rate: 0.00003125\tepoch: 882, Avg Loss: 2.4938, Learning Rate: 0.00003125\tepoch: 883, Avg Loss: 3.3886, Learning Rate: 0.00003125\tepoch: 884, Avg Loss: 2.7681, Learning Rate: 0.00003125\tepoch: 885, Avg Loss: 2.8202, Learning Rate: 0.00003125\tepoch: 886, Avg Loss: 2.5524, Learning Rate: 0.00003125\tepoch: 887, Avg Loss: 2.5960, Learning Rate: 0.00003125\tepoch: 888, Avg Loss: 3.0913, Learning Rate: 0.00003125\tepoch: 889, Avg Loss: 3.2207, Learning Rate: 0.00003125\tepoch: 890, Avg Loss: 2.7970, Learning Rate: 0.00003125\tepoch: 891, Avg Loss: 2.5068, Learning Rate: 0.00003125\tepoch: 892, Avg Loss: 2.3471, Learning Rate: 0.00003125\tepoch: 893, Avg Loss: 3.4473, Learning Rate: 0.00003125\tepoch: 894, Avg Loss: 2.7064, Learning Rate: 0.00003125\tepoch: 895, Avg Loss: 2.4662, Learning Rate: 0.00003125\tepoch: 896, Avg Loss: 2.8304, Learning Rate: 0.00003125\tepoch: 897, Avg Loss: 3.0166, Learning Rate: 0.00003125\tepoch: 898, Avg Loss: 3.0680, Learning Rate: 0.00003125\tepoch: 899, Avg Loss: 3.4013, Learning Rate: 0.00003125\tepoch: 900, Avg Loss: 2.9416, Learning Rate: 0.00003125\tepoch: 901, Avg Loss: 2.8932, Learning Rate: 0.00003125\tepoch: 902, Avg Loss: 2.3913, Learning Rate: 0.00003125\tepoch: 903, Avg Loss: 3.1146, Learning Rate: 0.00003125\tepoch: 904, Avg Loss: 3.4561, Learning Rate: 0.00003125\tepoch: 905, Avg Loss: 2.3195, Learning Rate: 0.00003125\tepoch: 906, Avg Loss: 2.2804, Learning Rate: 0.00003125\tepoch: 907, Avg Loss: 2.7594, Learning Rate: 0.00003125\tepoch: 908, Avg Loss: 2.8181, Learning Rate: 0.00003125\tepoch: 909, Avg Loss: 2.8491, Learning Rate: 0.00003125\tepoch: 910, Avg Loss: 3.5045, Learning Rate: 0.00003125\tepoch: 911, Avg Loss: 3.3780, Learning Rate: 0.00003125\tepoch: 912, Avg Loss: 2.5533, Learning Rate: 0.00003125\tepoch: 913, Avg Loss: 3.3745, Learning Rate: 0.00003125\tepoch: 914, Avg Loss: 2.6137, Learning Rate: 0.00003125\tepoch: 915, Avg Loss: 3.5562, Learning Rate: 0.00001563\tepoch: 916, Avg Loss: 3.5318, Learning Rate: 0.00001563\tepoch: 917, Avg Loss: 3.2279, Learning Rate: 0.00001563\tepoch: 918, Avg Loss: 2.6787, Learning Rate: 0.00001563\tepoch: 919, Avg Loss: 2.2762, Learning Rate: 0.00001563\tepoch: 920, Avg Loss: 3.2257, Learning Rate: 0.00001563\tepoch: 921, Avg Loss: 2.3914, Learning Rate: 0.00001563\tepoch: 922, Avg Loss: 2.9149, Learning Rate: 0.00001563\tepoch: 923, Avg Loss: 2.9616, Learning Rate: 0.00001563\tepoch: 924, Avg Loss: 2.6271, Learning Rate: 0.00001563\tepoch: 925, Avg Loss: 2.8125, Learning Rate: 0.00001563\tepoch: 926, Avg Loss: 2.9327, Learning Rate: 0.00001563\tepoch: 927, Avg Loss: 2.7471, Learning Rate: 0.00001563\tepoch: 928, Avg Loss: 2.7073, Learning Rate: 0.00001563\tepoch: 929, Avg Loss: 2.3413, Learning Rate: 0.00001563\tepoch: 930, Avg Loss: 3.3606, Learning Rate: 0.00001563\tepoch: 931, Avg Loss: 3.1932, Learning Rate: 0.00001563\tepoch: 932, Avg Loss: 2.5020, Learning Rate: 0.00001563\tepoch: 933, Avg Loss: 2.5526, Learning Rate: 0.00001563\tepoch: 934, Avg Loss: 2.5997, Learning Rate: 0.00001563\tepoch: 935, Avg Loss: 2.4005, Learning Rate: 0.00001563\tepoch: 936, Avg Loss: 3.7792, Learning Rate: 0.00001563\tepoch: 937, Avg Loss: 3.0454, Learning Rate: 0.00001563\tepoch: 938, Avg Loss: 3.0357, Learning Rate: 0.00001563\tepoch: 939, Avg Loss: 2.6117, Learning Rate: 0.00001563\tepoch: 940, Avg Loss: 2.4846, Learning Rate: 0.00001563\tepoch: 941, Avg Loss: 2.6434, Learning Rate: 0.00001563\tepoch: 942, Avg Loss: 2.6507, Learning Rate: 0.00001563\tepoch: 943, Avg Loss: 2.6454, Learning Rate: 0.00001563\tepoch: 944, Avg Loss: 3.3512, Learning Rate: 0.00001563\tepoch: 945, Avg Loss: 2.7430, Learning Rate: 0.00001563\tepoch: 946, Avg Loss: 3.3656, Learning Rate: 0.00001563\tepoch: 947, Avg Loss: 2.7461, Learning Rate: 0.00001563\tepoch: 948, Avg Loss: 2.4220, Learning Rate: 0.00001563\tepoch: 949, Avg Loss: 2.6073, Learning Rate: 0.00001563\tepoch: 950, Avg Loss: 3.5134, Learning Rate: 0.00001563\tepoch: 951, Avg Loss: 3.3251, Learning Rate: 0.00001563\tepoch: 952, Avg Loss: 3.5149, Learning Rate: 0.00001563\tepoch: 953, Avg Loss: 3.2782, Learning Rate: 0.00001563\tepoch: 954, Avg Loss: 2.3997, Learning Rate: 0.00001563\tepoch: 955, Avg Loss: 2.7845, Learning Rate: 0.00001563\tepoch: 956, Avg Loss: 3.2173, Learning Rate: 0.00001563\tepoch: 957, Avg Loss: 2.4394, Learning Rate: 0.00001563\tepoch: 958, Avg Loss: 2.4602, Learning Rate: 0.00001563\tepoch: 959, Avg Loss: 3.2066, Learning Rate: 0.00001563\tepoch: 960, Avg Loss: 3.6567, Learning Rate: 0.00001563\tepoch: 961, Avg Loss: 3.4139, Learning Rate: 0.00001563\tepoch: 962, Avg Loss: 2.6009, Learning Rate: 0.00001563\tepoch: 963, Avg Loss: 3.1906, Learning Rate: 0.00001563\tepoch: 964, Avg Loss: 2.5640, Learning Rate: 0.00001563\tepoch: 965, Avg Loss: 3.5104, Learning Rate: 0.00001563\tepoch: 966, Avg Loss: 3.5186, Learning Rate: 0.00000781\tepoch: 967, Avg Loss: 2.5877, Learning Rate: 0.00000781\tepoch: 968, Avg Loss: 2.5705, Learning Rate: 0.00000781\tepoch: 969, Avg Loss: 2.4398, Learning Rate: 0.00000781\tepoch: 970, Avg Loss: 2.9115, Learning Rate: 0.00000781\tepoch: 971, Avg Loss: 2.6399, Learning Rate: 0.00000781\tepoch: 972, Avg Loss: 3.0981, Learning Rate: 0.00000781\tepoch: 973, Avg Loss: 2.7328, Learning Rate: 0.00000781\tepoch: 974, Avg Loss: 2.7797, Learning Rate: 0.00000781\tepoch: 975, Avg Loss: 2.3489, Learning Rate: 0.00000781\tepoch: 976, Avg Loss: 2.6042, Learning Rate: 0.00000781\tepoch: 977, Avg Loss: 2.8798, Learning Rate: 0.00000781\tepoch: 978, Avg Loss: 2.5611, Learning Rate: 0.00000781\tepoch: 979, Avg Loss: 3.6443, Learning Rate: 0.00000781\tepoch: 980, Avg Loss: 3.3980, Learning Rate: 0.00000781\tepoch: 981, Avg Loss: 3.2613, Learning Rate: 0.00000781\tepoch: 982, Avg Loss: 2.5630, Learning Rate: 0.00000781\tepoch: 983, Avg Loss: 2.7904, Learning Rate: 0.00000781\tepoch: 984, Avg Loss: 3.1751, Learning Rate: 0.00000781\tepoch: 985, Avg Loss: 2.6882, Learning Rate: 0.00000781\tepoch: 986, Avg Loss: 2.4010, Learning Rate: 0.00000781\tepoch: 987, Avg Loss: 3.6066, Learning Rate: 0.00000781\tepoch: 988, Avg Loss: 3.9196, Learning Rate: 0.00000781\tepoch: 989, Avg Loss: 3.5072, Learning Rate: 0.00000781\tepoch: 990, Avg Loss: 3.1760, Learning Rate: 0.00000781\tepoch: 991, Avg Loss: 2.4964, Learning Rate: 0.00000781\tepoch: 992, Avg Loss: 2.3754, Learning Rate: 0.00000781\tepoch: 993, Avg Loss: 3.7537, Learning Rate: 0.00000781\tepoch: 994, Avg Loss: 2.6863, Learning Rate: 0.00000781\tepoch: 995, Avg Loss: 2.6783, Learning Rate: 0.00000781\tepoch: 996, Avg Loss: 3.6592, Learning Rate: 0.00000781\tepoch: 997, Avg Loss: 3.1961, Learning Rate: 0.00000781\tepoch: 998, Avg Loss: 2.3571, Learning Rate: 0.00000781\tepoch: 999, Avg Loss: 3.6764, Learning Rate: 0.00000781\tepoch: 1000, Avg Loss: 3.4529, Learning Rate: 0.00000781"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535b2b5a746641a88ccceeb3882aed78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "EXP = wuml.explainer(Udata, explainer_algorithm='shap')\n",
    "shap_values = EXP(Udata, output_all_results=False)\n",
    "print(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
